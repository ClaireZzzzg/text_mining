{"cells":[{"cell_type":"markdown","metadata":{"id":"EjSP3_nF-OXo"},"source":["# TEXT NORMALIZATION PRACTICE<br>\n","\n","\n","**<font color=green>INSTRUCTIONS:</font>** <br> <br>\n","    **<font color=green>1. Look for EXERCISES and QUESTIONS in this script. </font>** <br> <br>\n","    **<font color=green>2. Each student INDIVIDUALLY uploads this script with their answers embedded (and other materials if requested) to Canvas by the the deadline indicated on Canvas.</font>** <br>\n","## SESSION PREP\n","\n","### How to install any module from inside Jupyter\n","\n","To be able to install any module from inside Jupyper, we need module called sys:"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"qKqBY0kT-OXs"},"outputs":[],"source":["import sys"]},{"cell_type":"markdown","metadata":{"id":"ANmm0QG7-OXt"},"source":["Now, you can install any module from Jupyter by running a line such as: <br> <br> !{sys.executable} -m pip install module_name\n","\n","### Install Natural Language ToolKit (NLTK) module (and some other modules)\n","\n","The NLTK module does text normalization, among other functions. We'll install module NLTK, as well as modules numpy and pandas, from inside Jupyter (you might see deprication warnings in pink about future changes in the module but you do not need to pay attention to them at this time):"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!{sys.executable} -m pip install nltk\n","import nltk\n","\n","!{sys.executable} -m pip install numpy\n","import numpy as np \n","\n","!{sys.executable} -m pip install pandas\n","import pandas as pd"]},{"cell_type":"markdown","metadata":{"id":"MIpQQa9_-OXu"},"source":["## Download text data\n","\n","In what follows, we'll use an electronic archive of books from Project Gutenberg that Natural Language ToolKit has access to. In particular, we'll use \"Alice in Wonderland\" by Lewis Carrol. Our corpus will be just one file called carroll-alice.txt (it's in .txt format):"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":626,"status":"ok","timestamp":1648583227878,"user":{"displayName":"Claire Z.","userId":"15310629086553932750"},"user_tz":300},"id":"PeqM9UfH-OXu","outputId":"2b05d839-ed58-4642-e098-e3bacf825fce"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package gutenberg to /Users/kkee/nltk_data...\n"]},{"name":"stdout","output_type":"stream","text":["\"[Alice's Adventures in Wonderland b\"\n"]},{"name":"stderr","output_type":"stream","text":["[nltk_data]   Unzipping corpora/gutenberg.zip.\n"]}],"source":["nltk.download('gutenberg') \n","from nltk.corpus import gutenberg \n","\n","alice = gutenberg.raw(fileids='carroll-alice.txt') # we name the corpus 'alice'\n","from pprint import pprint #function for pretty printing\n","pprint(alice[0:35]) #print the first 35 characters of the corpus"]},{"cell_type":"markdown","metadata":{"id":"LGZlJqEY-OXv"},"source":["## TEXT TOKENIZATION\n","**Tokenization** is splitting text into sematically meaningful chuncks, such as sentences or words. Tokenizing into words is most common. You might be interested in tokenizing into sentences if you plan to analyze text sentence by sentence.\n","\n","### Tokenization by Sentence\n","From the NLTK module, we'll use a sentence tokenizer 'punkt':"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":523,"status":"ok","timestamp":1648583416825,"user":{"displayName":"Claire Z.","userId":"15310629086553932750"},"user_tz":300},"id":"TFjXJ77w-OXw","outputId":"b13cc4dd-85d4-4e09-9ded-c1b5c1b7516b"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package punkt to /Users/kkee/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"data":{"text/plain":["True"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["nltk.download('punkt')"]},{"cell_type":"markdown","metadata":{"id":"vF8GZCgf-OXx"},"source":["Let's now tokenize the Alice corpus by sentence:"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":244,"status":"ok","timestamp":1648583436917,"user":{"displayName":"Claire Z.","userId":"15310629086553932750"},"user_tz":300},"id":"Xthu6h1q-OXx","outputId":"55b70fdb-c45c-474a-bee1-a9ecb29066c6"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Total sentences in the corpus: 1625\n"]}],"source":["alice_sentences = nltk.sent_tokenize(text=alice)\n","print('\\nTotal sentences in the corpus:', len(alice_sentences))"]},{"cell_type":"markdown","metadata":{"id":"gHIVwpRi-OXy"},"source":["Let's have a look at the first sentence in the Alice corpus:"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":236,"status":"ok","timestamp":1648583446618,"user":{"displayName":"Claire Z.","userId":"15310629086553932750"},"user_tz":300},"id":"jWLkoAOz-OXy","outputId":"3a8c4983-df7a-47fd-97fe-7ceaef081af5"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","First sentence in alice: [Alice's Adventures in Wonderland by Lewis Carroll 1865]\n","\n","CHAPTER I.\n"]}],"source":["print('\\nFirst sentence in alice:', alice_sentences[0])"]},{"cell_type":"markdown","metadata":{"id":"4SA_pQ_T-OXz"},"source":["Let's now look at what the second sentence looks like:"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":101,"status":"ok","timestamp":1648583498860,"user":{"displayName":"Claire Z.","userId":"15310629086553932750"},"user_tz":300},"id":"zz8-Eip8-OXz","outputId":"87332dc1-566d-4458-afdf-d78c69f63dca"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Second sentence in alice: Down the Rabbit-Hole\n","\n","Alice was beginning to get very tired of sitting by her sister on the\n","bank, and of having nothing to do: once or twice she had peeped into the\n","book her sister was reading, but it had no pictures or conversations in\n","it, 'and what is the use of a book,' thought Alice 'without pictures or\n","conversation?'\n"]}],"source":["print('\\nSecond sentence in alice:', alice_sentences[1])"]},{"cell_type":"markdown","metadata":{"id":"id9Py2_E-OXz"},"source":["### <font color=green>QUESTION 1: Why do you think the first and second tokenized sentences (above) look like that? (look at what Python printed out)</font>"]},{"cell_type":"markdown","metadata":{"id":"cOtUSOgk-OX0"},"source":["### <font color=green> Answer:\n","\n","Because the sentence tokenization doesn't identify chapter title as a new line. It combines chapter title and the main body as one sentence as there are only spaces to seperate them.\n","  \n","### <font color=green> End of Answer"]},{"cell_type":"markdown","metadata":{"id":"XUQH-1kF-OX0"},"source":["### Tokenization into Words\n","Let's do some tokenization into words now. You can tokenize into words using punctuation signs, white spaces, or \"words\".\n","\n","We'll tokenize a corpus consisting of one sentence shown below:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8NMbTwRy-OX0"},"outputs":[],"source":["sentence = \"The brown fox wasn't that quick and he couldn't win the races\""]},{"cell_type":"markdown","metadata":{"id":"UB84_FbI-OX1"},"source":["Let's tokenize **using \"words\"**:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":115,"status":"ok","timestamp":1648584074554,"user":{"displayName":"Claire Z.","userId":"15310629086553932750"},"user_tz":300},"id":"YjF9hvEI-OX1","outputId":"b70f7c12-f833-4ca6-dc78-d020eec7e6a2"},"outputs":[{"name":"stdout","output_type":"stream","text":["['The', 'brown', 'fox', 'was', \"n't\", 'that', 'quick', 'and', 'he', 'could', \"n't\", 'win', 'the', 'races']\n"]}],"source":["words = nltk.word_tokenize(sentence)\n","print(words)  "]},{"cell_type":"markdown","metadata":{"id":"FYK_Gd1y-OX1"},"source":["Let's tokenize **using punctuation signs** now. Do you see any difference between this tokenization and the previous one?"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":109,"status":"ok","timestamp":1648584099539,"user":{"displayName":"Claire Z.","userId":"15310629086553932750"},"user_tz":300},"id":"DvubtW3n-OX1","outputId":"3fa11010-98b9-4a5b-9af9-c2ffd9a946b2"},"outputs":[{"name":"stdout","output_type":"stream","text":["['The', 'brown', 'fox', 'wasn', \"'\", 't', 'that', 'quick', 'and', 'he', 'couldn', \"'\", 't', 'win', 'the', 'races']\n"]}],"source":["wordpunkt_wt = nltk.WordPunctTokenizer()\n","words = wordpunkt_wt.tokenize(sentence)\n","print(words)"]},{"cell_type":"markdown","metadata":{"id":"D1TjcPQM-OX2"},"source":["Let's tokenize **using white spaces**:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":94,"status":"ok","timestamp":1648584127821,"user":{"displayName":"Claire Z.","userId":"15310629086553932750"},"user_tz":300},"id":"kz0csYfv-OX2","outputId":"657c9ef0-641b-448b-aef3-57a9aa30d6b5"},"outputs":[{"name":"stdout","output_type":"stream","text":["['The', 'brown', 'fox', \"wasn't\", 'that', 'quick', 'and', 'he', \"couldn't\", 'win', 'the', 'races']\n"]}],"source":["whitespace_wt = nltk.WhitespaceTokenizer()\n","words = whitespace_wt.tokenize(sentence)\n","print(words)"]},{"cell_type":"markdown","metadata":{"id":"YsK_fO7y-OX2"},"source":["## STOPWORDS"]},{"cell_type":"markdown","metadata":{"id":"MMet9VCl-OX2"},"source":["Let's get rid of stopwords (\"it's\", \"is\", \"the\", etc.):"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":132,"status":"ok","timestamp":1648584169305,"user":{"displayName":"Claire Z.","userId":"15310629086553932750"},"user_tz":300},"id":"l21W5Hew-OX2","outputId":"5419f257-c104-4871-c4c4-2e9a544b96d0"},"outputs":[{"name":"stdout","output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","{'can', 'mightn', 'same', 'than', 'all', \"aren't\", 'who', 'whom', 'out', 'will', 'which', 'your', 'about', 'own', \"hadn't\", 'yours', 'most', 'now', 'ours', 'from', 'during', 'wouldn', 'again', 'hers', 'some', 'nor', 'those', 'were', 'been', 'after', 'off', 'between', 'he', 'other', 'theirs', 'ma', \"shan't\", 'you', 'i', 'no', 'itself', 'few', \"you'd\", 'of', \"mustn't\", 'doing', \"wouldn't\", 'only', 'does', 'ourselves', 'a', 'his', 'just', 'her', 'with', 'once', 'their', 'doesn', 'o', 'before', \"weren't\", 'needn', \"that'll\", 'and', 'while', \"didn't\", 'shan', 'weren', 'we', 't', 'when', \"you've\", 'myself', 'was', 'herself', 'll', 'couldn', 'both', 'being', 'under', \"she's\", 'its', 'having', 'above', 'over', 'not', 'more', 'hasn', 've', 'yourself', 'through', 'down', 'against', \"won't\", 'in', \"mightn't\", 'are', 's', 'so', 'y', 'what', 'to', 'himself', 'very', 'such', 'did', 'them', 'didn', 'up', 'why', \"you're\", 'until', 'm', 'any', 'hadn', 're', \"isn't\", 'she', 'too', 'this', 'on', 'yourselves', 'but', 'if', 'how', 'won', 'me', 'for', \"it's\", 'wasn', 'an', 'then', 'do', \"hasn't\", \"you'll\", 'isn', 'has', \"should've\", 'further', 'by', 'into', 'themselves', 'don', 'each', \"wasn't\", 'should', 'have', 'the', 'at', 'they', 'as', \"don't\", \"couldn't\", 'it', 'our', 'that', 'ain', 'mustn', 'aren', 'haven', 'd', \"needn't\", 'him', 'there', 'am', 'is', \"shouldn't\", 'these', 'here', 'be', 'shouldn', 'where', 'my', \"haven't\", 'had', \"doesn't\", 'because', 'below', 'or'}\n"]}],"source":["from nltk.corpus import stopwords\n","nltk.download('stopwords')\n","\n","stop_words=set(stopwords.words(\"english\"))\n","print(stop_words)"]},{"cell_type":"markdown","metadata":{"id":"QAwmNdwK-OX3"},"source":["You can (and should consider) amending the list of stopwords given your data and project objectives. For example, we can add more stopwords to the standard list:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":102,"status":"ok","timestamp":1648584187788,"user":{"displayName":"Claire Z.","userId":"15310629086553932750"},"user_tz":300},"id":"lQIv06rz-OX3","outputId":"4db3d237-1a8b-4128-c4a6-a9c3288652ff"},"outputs":[{"name":"stdout","output_type":"stream","text":["{'can', 'mightn', 'same', 'than', 'all', \"aren't\", 'who', 'whom', 'out', 'will', 'which', 'your', 'about', 'own', \"hadn't\", 'yours', 'most', 'now', 'ours', 'from', 'during', 'wouldn', 'again', 'hers', 'some', 'nor', 'those', 'were', 'been', 'after', 'off', 'between', 'he', 'other', 'theirs', 'ma', \"shan't\", 'you', 'i', 'no', 'itself', 'few', \"you'd\", 'of', \"mustn't\", 'doing', \"wouldn't\", 'only', 'does', 'ourselves', 'a', 'his', 'just', 'her', 'with', 'once', 'their', 'doesn', 'o', 'before', \"weren't\", 'needn', \"that'll\", 'and', 'while', \"didn't\", 'shan', 'weren', 'we', 't', 'when', \"you've\", 'myself', 'was', 'herself', 'll', 'couldn', 'both', 'under', \"she's\", 'its', 'having', 'above', 'over', 'not', 'more', 'hasn', 've', 'yourself', 'through', 'down', 'against', \"won't\", 'in', \"mightn't\", 'are', 's', 'so', 'y', 'what', 'to', 'himself', 'because', 'very', 'such', 'did', 'them', 'didn', 'up', 'why', \"you're\", 'until', 'm', 'any', 'hadn', 're', \"isn't\", 'she', 'too', 'this', 'on', 'yourselves', 'but', 'if', 'how', 'won', 'me', 'for', \"it's\", 'wasn', 'an', 'then', 'do', \"hasn't\", \"you'll\", 'isn', 'has', \"should've\", 'further', 'by', 'into', 'themselves', 'don', 'each', \"wasn't\", 'should', 'NYC', 'have', 'the', 'at', 'they', 'as', \"don't\", \"couldn't\", 'it', 'our', 'that', 'ain', 'mustn', 'aren', 'haven', 'd', \"needn't\", 'him', 'there', 'am', 'is', \"shouldn't\", 'these', 'here', 'be', 'shouldn', 'where', 'my', \"haven't\", 'had', \"doesn't\", 'being', 'below', 'or'}\n"]}],"source":["add_stopwords ={'so','NYC'}\n","stop_words_new = add_stopwords.union(stop_words)\n","print(stop_words_new)"]},{"cell_type":"markdown","metadata":{"id":"PlwbWbIX-OX3"},"source":["Now, compare the tokenized sentence before and after removing the stopwords:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":103,"status":"ok","timestamp":1648584224475,"user":{"displayName":"Claire Z.","userId":"15310629086553932750"},"user_tz":300},"id":"0EAjpc1o-OX3","outputId":"d4d74601-1901-416d-c36a-cb577806baef"},"outputs":[{"name":"stdout","output_type":"stream","text":["Tokenized Sentence: ['The', 'brown', 'fox', \"wasn't\", 'that', 'quick', 'and', 'he', \"couldn't\", 'win', 'the', 'races']\n","Filterd Sentence (without stopwords): ['The', 'brown', 'fox', 'quick', 'win', 'races']\n"]}],"source":["filtered_tokens=[]\n","\n","for w in words:\n","    if w not in stop_words:\n","        filtered_tokens.append(w)\n","        \n","print(\"Tokenized Sentence:\",words)\n","print(\"Filterd Sentence (without stopwords):\",filtered_tokens)"]},{"cell_type":"markdown","metadata":{"id":"8YflJ3Eu-OX4"},"source":["## STEMMING AND LEMMATIZATION"]},{"cell_type":"markdown","metadata":{"id":"bT3DVnXq-OX4"},"source":["Let's stem the sentence first:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":114,"status":"ok","timestamp":1648584302434,"user":{"displayName":"Claire Z.","userId":"15310629086553932750"},"user_tz":300},"id":"89fuTEhn-OX4","outputId":"b200a04a-874d-4635-af52-7059b6fcca52"},"outputs":[{"name":"stdout","output_type":"stream","text":["Filtered Sentence: ['The', 'brown', 'fox', 'quick', 'win', 'races']\n","Stemmed Sentence: ['the', 'brown', 'fox', 'quick', 'win', 'race']\n"]}],"source":["from nltk.stem import PorterStemmer\n","\n","ps = PorterStemmer()\n","\n","stemmed_tokens=[]\n","for w in filtered_tokens:\n","    stemmed_tokens.append(ps.stem(w))\n","\n","print(\"Filtered Sentence:\",filtered_tokens)\n","print(\"Stemmed Sentence:\",stemmed_tokens)"]},{"cell_type":"markdown","metadata":{"id":"ysfE1qVu-OX4"},"source":["Compare stemming to lemmatization for the word \"running\": "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2242,"status":"ok","timestamp":1648584383685,"user":{"displayName":"Claire Z.","userId":"15310629086553932750"},"user_tz":300},"id":"XFR8fvN--OX4","outputId":"116aabcc-781b-434f-fb2b-e91f1716b8d8"},"outputs":[{"name":"stdout","output_type":"stream","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/wordnet.zip.\n","Lemmatized Word: run\n","Stemmed Word: run\n"]}],"source":["from nltk.stem.wordnet import WordNetLemmatizer\n","nltk.download('wordnet')\n","\n","lem = WordNetLemmatizer()\n","\n","from nltk.stem.porter import PorterStemmer\n","ps = PorterStemmer()\n","\n","word = \"running\"\n","print(\"Lemmatized Word:\",lem.lemmatize(word,\"v\")) # 'v' indicates that the word is a verb\n","print(\"Stemmed Word:\",ps.stem(word))"]},{"cell_type":"markdown","metadata":{"id":"Jus8LoHH-OX4"},"source":["One more comparison for the word \"bought\":"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":356,"status":"ok","timestamp":1648584391832,"user":{"displayName":"Claire Z.","userId":"15310629086553932750"},"user_tz":300},"id":"dknvasr8-OX5","outputId":"72aaea4e-8d40-4503-ef5a-e73317b65056"},"outputs":[{"name":"stdout","output_type":"stream","text":["Lemmatized Word: buy\n","Stemmed Word: bought\n"]}],"source":["word = \"bought\"\n","print(\"Lemmatized Word:\",lem.lemmatize(word,\"v\")) # 'v' indicates that the word is a verb (part-of-speech)\n","print(\"Stemmed Word:\",ps.stem(word))"]},{"cell_type":"markdown","metadata":{"id":"8Vj3JhjJ-OX5","tags":[]},"source":["### <font color=green>EXERCISE 1: What result would you get if you change the part-of-speech tag in the lemmatization line above to \"n\", which means \"noun\"? (look at what Python printed out)</font> <br>\n","### <font color=green> Answer:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":265,"status":"ok","timestamp":1648584514634,"user":{"displayName":"Claire Z.","userId":"15310629086553932750"},"user_tz":300},"id":"RU3gWJHo-OX5","outputId":"8eb9b67e-d56a-472f-f886-ff99011ded9e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Lemmatized Word: bought\n","Stemmed Word: bought\n"]}],"source":["word = \"bought\"\n","print(\"Lemmatized Word:\",lem.lemmatize(word,\"n\")) \n","print(\"Stemmed Word:\",ps.stem(word))"]},{"cell_type":"markdown","metadata":{"id":"zuhgA4lVTArU"},"source":["Because if the machine recognizes the word we input as a regular verb, stemming and lemmazatization can convert it to original form. If the machine recognizes the word we input as an irregular verb, only lemmazatization can successfully change it to the original form. And Both stemming and lemmazatization cannot convert a noun to anything."]},{"cell_type":"markdown","metadata":{"id":"nCf20CG1-OX5"},"source":[]},{"cell_type":"markdown","metadata":{"id":"b7DEot9N-OX5"},"source":["### <font color=green> End of Answer"]}],"metadata":{"colab":{"collapsed_sections":["cOtUSOgk-OX0","b7DEot9N-OX5","io0YSTri-OX8","oNPIe3cL-OX8","ijqHSdF5-OYA"],"name":"Lab_2_Normalization_and_Vectorization_S2022.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3.9.9 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.9"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false},"vscode":{"interpreter":{"hash":"aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"}}},"nbformat":4,"nbformat_minor":0}
