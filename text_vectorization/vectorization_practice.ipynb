{"cells":[{"cell_type":"markdown","metadata":{"id":"EjSP3_nF-OXo"},"source":["# Lab 2: TEXT NORMALIZATION and VECTORIZATION <br>\n","\n","\n","**<font color=green>INSTRUCTIONS:</font>** <br> <br>\n","    **<font color=green>1. Look for EXERCISES and QUESTIONS in this script. </font>** <br> <br>\n","    **<font color=green>2. Each student INDIVIDUALLY uploads this script with their answers embedded (and other materials if requested) to Canvas by the the deadline indicated on Canvas.</font>** <br>\n","## SESSION PREP\n","\n","### How to install any module from inside Jupyter\n","\n","To be able to install any module from inside Jupyper, we need module called sys:"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"qKqBY0kT-OXs"},"outputs":[],"source":["import sys"]},{"cell_type":"markdown","metadata":{"id":"ANmm0QG7-OXt"},"source":["Now, you can install any module from Jupyter by running a line such as: <br> <br> !{sys.executable} -m pip install module_name\n","\n","### Install Natural Language ToolKit (NLTK) module (and some other modules)\n","\n","The NLTK module does text normalization, among other functions. We'll install module NLTK, as well as modules numpy and pandas, from inside Jupyter (you might see deprication warnings in pink about future changes in the module but you do not need to pay attention to them at this time):"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15058,"status":"ok","timestamp":1648583216465,"user":{"displayName":"Claire Z.","userId":"15310629086553932750"},"user_tz":300},"id":"uaLa2pGI-OXt","outputId":"bb63d74a-e756-44d5-f995-3b7fc2b28e80","scrolled":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\n","Collecting nltk\n","  Downloading nltk-3.7-py3-none-any.whl (1.5 MB)\n","     |████████████████████████████████| 1.5 MB 1.4 MB/s            \n","\u001b[?25hCollecting regex>=2021.8.3\n","  Downloading regex-2022.7.25-cp39-cp39-macosx_10_9_x86_64.whl (289 kB)\n","     |████████████████████████████████| 289 kB 33.7 MB/s            \n","\u001b[?25hCollecting joblib\n","  Downloading joblib-1.1.0-py2.py3-none-any.whl (306 kB)\n","     |████████████████████████████████| 306 kB 53.0 MB/s            \n","\u001b[?25hCollecting tqdm\n","  Downloading tqdm-4.64.0-py2.py3-none-any.whl (78 kB)\n","     |████████████████████████████████| 78 kB 15.7 MB/s            \n","\u001b[?25hCollecting click\n","  Downloading click-8.1.3-py3-none-any.whl (96 kB)\n","     |████████████████████████████████| 96 kB 14.5 MB/s            \n","\u001b[?25hInstalling collected packages: tqdm, regex, joblib, click, nltk\n","\u001b[33m  DEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\n","\u001b[33m  DEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\n","\u001b[33m  DEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\n","\u001b[33m  DEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\n","\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\n","Successfully installed click-8.1.3 joblib-1.1.0 nltk-3.7 regex-2022.7.25 tqdm-4.64.0\n","\u001b[33mWARNING: You are using pip version 21.3.1; however, version 22.2.2 is available.\n","You should consider upgrading via the '/usr/local/opt/python@3.9/bin/python3.9 -m pip install --upgrade pip' command.\u001b[0m\n","\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\n","Collecting numpy\n","  Downloading numpy-1.23.1-cp39-cp39-macosx_10_9_x86_64.whl (18.1 MB)\n","     |████████████████████████████████| 18.1 MB 1.1 MB/s            \n","\u001b[?25hInstalling collected packages: numpy\n","\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\n","Successfully installed numpy-1.23.1\n","\u001b[33mWARNING: You are using pip version 21.3.1; however, version 22.2.2 is available.\n","You should consider upgrading via the '/usr/local/opt/python@3.9/bin/python3.9 -m pip install --upgrade pip' command.\u001b[0m\n","\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\n","Collecting pandas\n","  Downloading pandas-1.4.3-cp39-cp39-macosx_10_9_x86_64.whl (11.5 MB)\n","     |████████████████████████████████| 11.5 MB 1.3 MB/s            \n","\u001b[?25hRequirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.9/site-packages (from pandas) (1.23.1)\n","Collecting pytz>=2020.1\n","  Downloading pytz-2022.1-py2.py3-none-any.whl (503 kB)\n","     |████████████████████████████████| 503 kB 26.5 MB/s            \n","\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/site-packages (from pandas) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n","Installing collected packages: pytz, pandas\n","\u001b[33m  DEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\n","\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\n","Successfully installed pandas-1.4.3 pytz-2022.1\n","\u001b[33mWARNING: You are using pip version 21.3.1; however, version 22.2.2 is available.\n","You should consider upgrading via the '/usr/local/opt/python@3.9/bin/python3.9 -m pip install --upgrade pip' command.\u001b[0m\n"]}],"source":["!{sys.executable} -m pip install nltk\n","import nltk\n","\n","!{sys.executable} -m pip install numpy\n","import numpy as np \n","\n","!{sys.executable} -m pip install pandas\n","import pandas as pd"]},{"cell_type":"markdown","metadata":{"id":"MIpQQa9_-OXu"},"source":["## Download text data\n","\n","In what follows, we'll use an electronic archive of books from Project Gutenberg that Natural Language ToolKit has access to. In particular, we'll use \"Alice in Wonderland\" by Lewis Carrol. Our corpus will be just one file called carroll-alice.txt (it's in .txt format):"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":626,"status":"ok","timestamp":1648583227878,"user":{"displayName":"Claire Z.","userId":"15310629086553932750"},"user_tz":300},"id":"PeqM9UfH-OXu","outputId":"2b05d839-ed58-4642-e098-e3bacf825fce"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package gutenberg to /Users/kkee/nltk_data...\n"]},{"name":"stdout","output_type":"stream","text":["\"[Alice's Adventures in Wonderland b\"\n"]},{"name":"stderr","output_type":"stream","text":["[nltk_data]   Unzipping corpora/gutenberg.zip.\n"]}],"source":["nltk.download('gutenberg') \n","from nltk.corpus import gutenberg \n","\n","alice = gutenberg.raw(fileids='carroll-alice.txt') # we name the corpus 'alice'\n","from pprint import pprint #function for pretty printing\n","pprint(alice[0:35]) #print the first 35 characters of the corpus"]},{"cell_type":"markdown","metadata":{"id":"LGZlJqEY-OXv"},"source":["## TEXT TOKENIZATION\n","**Tokenization** is splitting text into sematically meaningful chuncks, such as sentences or words. Tokenizing into words is most common. You might be interested in tokenizing into sentences if you plan to analyze text sentence by sentence.\n","\n","### Tokenization by Sentence\n","From the NLTK module, we'll use a sentence tokenizer 'punkt':"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":523,"status":"ok","timestamp":1648583416825,"user":{"displayName":"Claire Z.","userId":"15310629086553932750"},"user_tz":300},"id":"TFjXJ77w-OXw","outputId":"b13cc4dd-85d4-4e09-9ded-c1b5c1b7516b"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package punkt to /Users/kkee/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"data":{"text/plain":["True"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["nltk.download('punkt')"]},{"cell_type":"markdown","metadata":{"id":"vF8GZCgf-OXx"},"source":["Let's now tokenize the Alice corpus by sentence:"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":244,"status":"ok","timestamp":1648583436917,"user":{"displayName":"Claire Z.","userId":"15310629086553932750"},"user_tz":300},"id":"Xthu6h1q-OXx","outputId":"55b70fdb-c45c-474a-bee1-a9ecb29066c6"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Total sentences in the corpus: 1625\n"]}],"source":["alice_sentences = nltk.sent_tokenize(text=alice)\n","print('\\nTotal sentences in the corpus:', len(alice_sentences))"]},{"cell_type":"markdown","metadata":{"id":"gHIVwpRi-OXy"},"source":["Let's have a look at the first sentence in the Alice corpus:"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":236,"status":"ok","timestamp":1648583446618,"user":{"displayName":"Claire Z.","userId":"15310629086553932750"},"user_tz":300},"id":"jWLkoAOz-OXy","outputId":"3a8c4983-df7a-47fd-97fe-7ceaef081af5"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","First sentence in alice: [Alice's Adventures in Wonderland by Lewis Carroll 1865]\n","\n","CHAPTER I.\n"]}],"source":["print('\\nFirst sentence in alice:', alice_sentences[0])"]},{"cell_type":"markdown","metadata":{"id":"4SA_pQ_T-OXz"},"source":["Let's now look at what the second sentence looks like:"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":101,"status":"ok","timestamp":1648583498860,"user":{"displayName":"Claire Z.","userId":"15310629086553932750"},"user_tz":300},"id":"zz8-Eip8-OXz","outputId":"87332dc1-566d-4458-afdf-d78c69f63dca"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Second sentence in alice: Down the Rabbit-Hole\n","\n","Alice was beginning to get very tired of sitting by her sister on the\n","bank, and of having nothing to do: once or twice she had peeped into the\n","book her sister was reading, but it had no pictures or conversations in\n","it, 'and what is the use of a book,' thought Alice 'without pictures or\n","conversation?'\n"]}],"source":["print('\\nSecond sentence in alice:', alice_sentences[1])"]},{"cell_type":"markdown","metadata":{"id":"id9Py2_E-OXz"},"source":["### <font color=green>QUESTION 1: Why do you think the first and second tokenized sentences (above) look like that? (look at what Python printed out)</font>"]},{"cell_type":"markdown","metadata":{"id":"cOtUSOgk-OX0"},"source":["### <font color=green> Answer:\n","\n","Because the sentence tokenization doesn't identify chapter title as a new line. It combines chapter title and the main body as one sentence as there are only spaces to seperate them.\n","  \n","### <font color=green> End of Answer"]},{"cell_type":"markdown","metadata":{"id":"XUQH-1kF-OX0"},"source":["### Tokenization into Words\n","Let's do some tokenization into words now. You can tokenize into words using punctuation signs, white spaces, or \"words\".\n","\n","We'll tokenize a corpus consisting of one sentence shown below:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8NMbTwRy-OX0"},"outputs":[],"source":["sentence = \"The brown fox wasn't that quick and he couldn't win the races\""]},{"cell_type":"markdown","metadata":{"id":"UB84_FbI-OX1"},"source":["Let's tokenize **using \"words\"**:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":115,"status":"ok","timestamp":1648584074554,"user":{"displayName":"Claire Z.","userId":"15310629086553932750"},"user_tz":300},"id":"YjF9hvEI-OX1","outputId":"b70f7c12-f833-4ca6-dc78-d020eec7e6a2"},"outputs":[{"name":"stdout","output_type":"stream","text":["['The', 'brown', 'fox', 'was', \"n't\", 'that', 'quick', 'and', 'he', 'could', \"n't\", 'win', 'the', 'races']\n"]}],"source":["words = nltk.word_tokenize(sentence)\n","print(words)  "]},{"cell_type":"markdown","metadata":{"id":"FYK_Gd1y-OX1"},"source":["Let's tokenize **using punctuation signs** now. Do you see any difference between this tokenization and the previous one?"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":109,"status":"ok","timestamp":1648584099539,"user":{"displayName":"Claire Z.","userId":"15310629086553932750"},"user_tz":300},"id":"DvubtW3n-OX1","outputId":"3fa11010-98b9-4a5b-9af9-c2ffd9a946b2"},"outputs":[{"name":"stdout","output_type":"stream","text":["['The', 'brown', 'fox', 'wasn', \"'\", 't', 'that', 'quick', 'and', 'he', 'couldn', \"'\", 't', 'win', 'the', 'races']\n"]}],"source":["wordpunkt_wt = nltk.WordPunctTokenizer()\n","words = wordpunkt_wt.tokenize(sentence)\n","print(words)"]},{"cell_type":"markdown","metadata":{"id":"D1TjcPQM-OX2"},"source":["Let's tokenize **using white spaces**:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":94,"status":"ok","timestamp":1648584127821,"user":{"displayName":"Claire Z.","userId":"15310629086553932750"},"user_tz":300},"id":"kz0csYfv-OX2","outputId":"657c9ef0-641b-448b-aef3-57a9aa30d6b5"},"outputs":[{"name":"stdout","output_type":"stream","text":["['The', 'brown', 'fox', \"wasn't\", 'that', 'quick', 'and', 'he', \"couldn't\", 'win', 'the', 'races']\n"]}],"source":["whitespace_wt = nltk.WhitespaceTokenizer()\n","words = whitespace_wt.tokenize(sentence)\n","print(words)"]},{"cell_type":"markdown","metadata":{"id":"YsK_fO7y-OX2"},"source":["## STOPWORDS"]},{"cell_type":"markdown","metadata":{"id":"MMet9VCl-OX2"},"source":["Let's get rid of stopwords (\"it's\", \"is\", \"the\", etc.):"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":132,"status":"ok","timestamp":1648584169305,"user":{"displayName":"Claire Z.","userId":"15310629086553932750"},"user_tz":300},"id":"l21W5Hew-OX2","outputId":"5419f257-c104-4871-c4c4-2e9a544b96d0"},"outputs":[{"name":"stdout","output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","{'can', 'mightn', 'same', 'than', 'all', \"aren't\", 'who', 'whom', 'out', 'will', 'which', 'your', 'about', 'own', \"hadn't\", 'yours', 'most', 'now', 'ours', 'from', 'during', 'wouldn', 'again', 'hers', 'some', 'nor', 'those', 'were', 'been', 'after', 'off', 'between', 'he', 'other', 'theirs', 'ma', \"shan't\", 'you', 'i', 'no', 'itself', 'few', \"you'd\", 'of', \"mustn't\", 'doing', \"wouldn't\", 'only', 'does', 'ourselves', 'a', 'his', 'just', 'her', 'with', 'once', 'their', 'doesn', 'o', 'before', \"weren't\", 'needn', \"that'll\", 'and', 'while', \"didn't\", 'shan', 'weren', 'we', 't', 'when', \"you've\", 'myself', 'was', 'herself', 'll', 'couldn', 'both', 'being', 'under', \"she's\", 'its', 'having', 'above', 'over', 'not', 'more', 'hasn', 've', 'yourself', 'through', 'down', 'against', \"won't\", 'in', \"mightn't\", 'are', 's', 'so', 'y', 'what', 'to', 'himself', 'very', 'such', 'did', 'them', 'didn', 'up', 'why', \"you're\", 'until', 'm', 'any', 'hadn', 're', \"isn't\", 'she', 'too', 'this', 'on', 'yourselves', 'but', 'if', 'how', 'won', 'me', 'for', \"it's\", 'wasn', 'an', 'then', 'do', \"hasn't\", \"you'll\", 'isn', 'has', \"should've\", 'further', 'by', 'into', 'themselves', 'don', 'each', \"wasn't\", 'should', 'have', 'the', 'at', 'they', 'as', \"don't\", \"couldn't\", 'it', 'our', 'that', 'ain', 'mustn', 'aren', 'haven', 'd', \"needn't\", 'him', 'there', 'am', 'is', \"shouldn't\", 'these', 'here', 'be', 'shouldn', 'where', 'my', \"haven't\", 'had', \"doesn't\", 'because', 'below', 'or'}\n"]}],"source":["from nltk.corpus import stopwords\n","nltk.download('stopwords')\n","\n","stop_words=set(stopwords.words(\"english\"))\n","print(stop_words)"]},{"cell_type":"markdown","metadata":{"id":"QAwmNdwK-OX3"},"source":["You can (and should consider) amending the list of stopwords given your data and project objectives. For example, we can add more stopwords to the standard list:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":102,"status":"ok","timestamp":1648584187788,"user":{"displayName":"Claire Z.","userId":"15310629086553932750"},"user_tz":300},"id":"lQIv06rz-OX3","outputId":"4db3d237-1a8b-4128-c4a6-a9c3288652ff"},"outputs":[{"name":"stdout","output_type":"stream","text":["{'can', 'mightn', 'same', 'than', 'all', \"aren't\", 'who', 'whom', 'out', 'will', 'which', 'your', 'about', 'own', \"hadn't\", 'yours', 'most', 'now', 'ours', 'from', 'during', 'wouldn', 'again', 'hers', 'some', 'nor', 'those', 'were', 'been', 'after', 'off', 'between', 'he', 'other', 'theirs', 'ma', \"shan't\", 'you', 'i', 'no', 'itself', 'few', \"you'd\", 'of', \"mustn't\", 'doing', \"wouldn't\", 'only', 'does', 'ourselves', 'a', 'his', 'just', 'her', 'with', 'once', 'their', 'doesn', 'o', 'before', \"weren't\", 'needn', \"that'll\", 'and', 'while', \"didn't\", 'shan', 'weren', 'we', 't', 'when', \"you've\", 'myself', 'was', 'herself', 'll', 'couldn', 'both', 'under', \"she's\", 'its', 'having', 'above', 'over', 'not', 'more', 'hasn', 've', 'yourself', 'through', 'down', 'against', \"won't\", 'in', \"mightn't\", 'are', 's', 'so', 'y', 'what', 'to', 'himself', 'because', 'very', 'such', 'did', 'them', 'didn', 'up', 'why', \"you're\", 'until', 'm', 'any', 'hadn', 're', \"isn't\", 'she', 'too', 'this', 'on', 'yourselves', 'but', 'if', 'how', 'won', 'me', 'for', \"it's\", 'wasn', 'an', 'then', 'do', \"hasn't\", \"you'll\", 'isn', 'has', \"should've\", 'further', 'by', 'into', 'themselves', 'don', 'each', \"wasn't\", 'should', 'NYC', 'have', 'the', 'at', 'they', 'as', \"don't\", \"couldn't\", 'it', 'our', 'that', 'ain', 'mustn', 'aren', 'haven', 'd', \"needn't\", 'him', 'there', 'am', 'is', \"shouldn't\", 'these', 'here', 'be', 'shouldn', 'where', 'my', \"haven't\", 'had', \"doesn't\", 'being', 'below', 'or'}\n"]}],"source":["add_stopwords ={'so','NYC'}\n","stop_words_new = add_stopwords.union(stop_words)\n","print(stop_words_new)"]},{"cell_type":"markdown","metadata":{"id":"PlwbWbIX-OX3"},"source":["Now, compare the tokenized sentence before and after removing the stopwords:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":103,"status":"ok","timestamp":1648584224475,"user":{"displayName":"Claire Z.","userId":"15310629086553932750"},"user_tz":300},"id":"0EAjpc1o-OX3","outputId":"d4d74601-1901-416d-c36a-cb577806baef"},"outputs":[{"name":"stdout","output_type":"stream","text":["Tokenized Sentence: ['The', 'brown', 'fox', \"wasn't\", 'that', 'quick', 'and', 'he', \"couldn't\", 'win', 'the', 'races']\n","Filterd Sentence (without stopwords): ['The', 'brown', 'fox', 'quick', 'win', 'races']\n"]}],"source":["filtered_tokens=[]\n","\n","for w in words:\n","    if w not in stop_words:\n","        filtered_tokens.append(w)\n","        \n","print(\"Tokenized Sentence:\",words)\n","print(\"Filterd Sentence (without stopwords):\",filtered_tokens)"]},{"cell_type":"markdown","metadata":{"id":"8YflJ3Eu-OX4"},"source":["## STEMMING AND LEMMATIZATION"]},{"cell_type":"markdown","metadata":{"id":"bT3DVnXq-OX4"},"source":["Let's stem the sentence first:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":114,"status":"ok","timestamp":1648584302434,"user":{"displayName":"Claire Z.","userId":"15310629086553932750"},"user_tz":300},"id":"89fuTEhn-OX4","outputId":"b200a04a-874d-4635-af52-7059b6fcca52"},"outputs":[{"name":"stdout","output_type":"stream","text":["Filtered Sentence: ['The', 'brown', 'fox', 'quick', 'win', 'races']\n","Stemmed Sentence: ['the', 'brown', 'fox', 'quick', 'win', 'race']\n"]}],"source":["from nltk.stem import PorterStemmer\n","\n","ps = PorterStemmer()\n","\n","stemmed_tokens=[]\n","for w in filtered_tokens:\n","    stemmed_tokens.append(ps.stem(w))\n","\n","print(\"Filtered Sentence:\",filtered_tokens)\n","print(\"Stemmed Sentence:\",stemmed_tokens)"]},{"cell_type":"markdown","metadata":{"id":"ysfE1qVu-OX4"},"source":["Compare stemming to lemmatization for the word \"running\": "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2242,"status":"ok","timestamp":1648584383685,"user":{"displayName":"Claire Z.","userId":"15310629086553932750"},"user_tz":300},"id":"XFR8fvN--OX4","outputId":"116aabcc-781b-434f-fb2b-e91f1716b8d8"},"outputs":[{"name":"stdout","output_type":"stream","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/wordnet.zip.\n","Lemmatized Word: run\n","Stemmed Word: run\n"]}],"source":["from nltk.stem.wordnet import WordNetLemmatizer\n","nltk.download('wordnet')\n","\n","lem = WordNetLemmatizer()\n","\n","from nltk.stem.porter import PorterStemmer\n","ps = PorterStemmer()\n","\n","word = \"running\"\n","print(\"Lemmatized Word:\",lem.lemmatize(word,\"v\")) # 'v' indicates that the word is a verb\n","print(\"Stemmed Word:\",ps.stem(word))"]},{"cell_type":"markdown","metadata":{"id":"Jus8LoHH-OX4"},"source":["One more comparison for the word \"bought\":"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":356,"status":"ok","timestamp":1648584391832,"user":{"displayName":"Claire Z.","userId":"15310629086553932750"},"user_tz":300},"id":"dknvasr8-OX5","outputId":"72aaea4e-8d40-4503-ef5a-e73317b65056"},"outputs":[{"name":"stdout","output_type":"stream","text":["Lemmatized Word: buy\n","Stemmed Word: bought\n"]}],"source":["word = \"bought\"\n","print(\"Lemmatized Word:\",lem.lemmatize(word,\"v\")) # 'v' indicates that the word is a verb (part-of-speech)\n","print(\"Stemmed Word:\",ps.stem(word))"]},{"cell_type":"markdown","metadata":{"id":"8Vj3JhjJ-OX5","tags":[]},"source":["### <font color=green>EXERCISE 1: What result would you get if you change the part-of-speech tag in the lemmatization line above to \"n\", which means \"noun\"? (look at what Python printed out)</font> <br>\n","### <font color=green> Answer:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":265,"status":"ok","timestamp":1648584514634,"user":{"displayName":"Claire Z.","userId":"15310629086553932750"},"user_tz":300},"id":"RU3gWJHo-OX5","outputId":"8eb9b67e-d56a-472f-f886-ff99011ded9e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Lemmatized Word: bought\n","Stemmed Word: bought\n"]}],"source":["word = \"bought\"\n","print(\"Lemmatized Word:\",lem.lemmatize(word,\"n\")) \n","print(\"Stemmed Word:\",ps.stem(word))"]},{"cell_type":"markdown","metadata":{"id":"zuhgA4lVTArU"},"source":["Because if the machine recognizes the word we input as a regular verb, stemming and lemmazatization can convert it to original form. If the machine recognizes the word we input as an irregular verb, only lemmazatization can successfully change it to the original form. And Both stemming and lemmazatization cannot convert a noun to anything."]},{"cell_type":"markdown","metadata":{"id":"nCf20CG1-OX5"},"source":[]},{"cell_type":"markdown","metadata":{"id":"b7DEot9N-OX5"},"source":["### <font color=green> End of Answer"]},{"cell_type":"markdown","metadata":{"id":"4HxZwv2Z-OX5"},"source":["## VECTORIZATION"]},{"cell_type":"markdown","metadata":{"id":"3tUYy5Yo-OX6"},"source":["Text vectorization is the process of feature extraction from text data, that is the process of creating variables for each observation, where an observation is a text document. We'll consider the **bag-of-words**, the **TF-IDF** and the **n-grams** vectorized representations of text. <br>\n","\n","Let's vectorize the corpus about \"blue skies and blue cheese\" similar to one used in the video lecture: "]},{"cell_type":"code","execution_count":8,"metadata":{"id":"1es07270-OX6"},"outputs":[],"source":["corpus = ['the sky is blue',\n","          'sky is blue and sky is beautiful', \n","          'the beautiful sky is so blue',\n","          'i love blue cheese']"]},{"cell_type":"markdown","metadata":{"id":"fZNw2dzc-OX6"},"source":["We'll use built-in vectorizers from Scikit-Learn module for machine learning. "]},{"cell_type":"markdown","metadata":{"id":"5AbVrZT2-OX6"},"source":["### Bag-of-Words Representation\n","\n","We'll use bag-of-words representation (CountVectorizer) first. You can see the documentation here:\n","https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"VJlZj8RN-OX6"},"outputs":[],"source":["from sklearn.feature_extraction.text import CountVectorizer\n","vectorizer_BOW = CountVectorizer(max_features=1000) #BOW = bag-of-words\n","BOW_matrix = vectorizer_BOW.fit_transform(corpus).toarray()\n","pd.DataFrame(np.round(BOW_matrix,2),columns=vectorizer_BOW.get_feature_names())"]},{"cell_type":"markdown","metadata":{},"source":["Let's break it down:"]},{"cell_type":"markdown","metadata":{"id":"9tKTlb7j-OX6"},"source":["It is convinient to \"define\" a vectorizer first before applying it. You can specify all the parameters (arguments) of the function in the definition. For example, the max_features parameter below drops all features except for the selected number of most frequent terms in the corpus:"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"ioIrF0LW-OX6"},"outputs":[],"source":["vectorizer_BOW = CountVectorizer(max_features=1000) #BOW = bag-of-words"]},{"cell_type":"markdown","metadata":{"id":"G2CjlNR_-OX7"},"source":["Now let's extract features using the vectorizer function. Note the .fit_transform function below. It creates the dictionary of the corpus and does the vectorization: "]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":175},"executionInfo":{"elapsed":173,"status":"ok","timestamp":1648584867163,"user":{"displayName":"Claire Z.","userId":"15310629086553932750"},"user_tz":300},"id":"3NtgxEv1-OX7","outputId":"c0b96314-b5cb-430b-a1f2-f822409b80dd"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","      <th>3</th>\n","      <th>4</th>\n","      <th>5</th>\n","      <th>6</th>\n","      <th>7</th>\n","      <th>8</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   0  1  2  3  4  5  6  7  8\n","0  0  0  1  0  1  0  1  0  1\n","1  1  1  1  0  2  0  2  0  0\n","2  0  1  1  0  1  0  1  1  1\n","3  0  0  1  1  0  1  0  0  0"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["BOW_matrix = vectorizer_BOW.fit_transform(corpus).toarray()\n","pd.DataFrame(np.round(BOW_matrix,2))"]},{"cell_type":"markdown","metadata":{"id":"ePTXazlj-OX7"},"source":["We want to attach the names of the features, right? Here are the names of the features from the dictionary of the corpus (note the function get_feature_names()):"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":96,"status":"ok","timestamp":1648584901918,"user":{"displayName":"Claire Z.","userId":"15310629086553932750"},"user_tz":300},"id":"YaLBRyvt-OX7","outputId":"376e3d4d-1413-4efa-808b-0f8cc7bf235d"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n","  warnings.warn(msg, category=FutureWarning)\n"]},{"data":{"text/plain":["['and', 'beautiful', 'blue', 'cheese', 'is', 'love', 'sky', 'so', 'the']"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["vectorizer_BOW.get_feature_names()"]},{"cell_type":"markdown","metadata":{"id":"kPr9Uxg--OX7"},"source":["Let's get a more useful looking bag-of-words representation, with feature names attached:"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":248},"executionInfo":{"elapsed":110,"status":"ok","timestamp":1648584911531,"user":{"displayName":"Claire Z.","userId":"15310629086553932750"},"user_tz":300},"id":"36fXTN6_-OX8","outputId":"60d3ffd9-fdc3-4bcc-990c-92d8fe5acace"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>and</th>\n","      <th>beautiful</th>\n","      <th>blue</th>\n","      <th>cheese</th>\n","      <th>is</th>\n","      <th>love</th>\n","      <th>sky</th>\n","      <th>so</th>\n","      <th>the</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   and  beautiful  blue  cheese  is  love  sky  so  the\n","0    0          0     1       0   1     0    1   0    1\n","1    1          1     1       0   2     0    2   0    0\n","2    0          1     1       0   1     0    1   1    1\n","3    0          0     1       1   0     1    0   0    0"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["pd.DataFrame(np.round(BOW_matrix,2),columns=vectorizer_BOW.get_feature_names())"]},{"cell_type":"markdown","metadata":{"id":"io0YSTri-OX8"},"source":["### Vectorization Using N-grams\n","<br>\n","Let's use bi-grams in our vectorized representation of text. First, we define the vectorizer (we need the same CountVectorizer() function) using a parameter for specifying n-grams. Then we apply it:"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":231},"executionInfo":{"elapsed":108,"status":"ok","timestamp":1648584954026,"user":{"displayName":"Claire Z.","userId":"15310629086553932750"},"user_tz":300},"id":"g5vT71xA-OX8","outputId":"dfb9f98a-73bd-4db0-a6f2-8b3e17404a52"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n","  warnings.warn(msg, category=FutureWarning)\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>and sky</th>\n","      <th>beautiful sky</th>\n","      <th>blue and</th>\n","      <th>blue cheese</th>\n","      <th>is beautiful</th>\n","      <th>is blue</th>\n","      <th>is so</th>\n","      <th>love blue</th>\n","      <th>sky is</th>\n","      <th>so blue</th>\n","      <th>the beautiful</th>\n","      <th>the sky</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   and sky  beautiful sky  blue and  blue cheese  is beautiful  is blue  \\\n","0        0              0         0            0             0        1   \n","1        1              0         1            0             1        1   \n","2        0              1         0            0             0        0   \n","3        0              0         0            1             0        0   \n","\n","   is so  love blue  sky is  so blue  the beautiful  the sky  \n","0      0          0       1        0              0        1  \n","1      0          0       2        0              0        0  \n","2      1          0       1        1              1        0  \n","3      0          1       0        0              0        0  "]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["vectorizer_Bi_Grams = CountVectorizer(max_features=1000, ngram_range=(2, 2))\n","Bi_Grams_matrix = vectorizer_Bi_Grams.fit_transform(corpus).toarray()\n","pd.DataFrame(np.round(Bi_Grams_matrix,2),columns=vectorizer_Bi_Grams.get_feature_names())"]},{"cell_type":"markdown","metadata":{"id":"K4oguVnZ-OX8"},"source":["### <font color=green>EXERCISE 2: Create a Bi-Grams vectorizer that uses the mix of bi-grams and uni-grams. To complete the Exercise you may need to look up CountVectorizer's documentation, see link below.</font> <br>\n","\n","Documentation: \n","https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html <br>\n","\n","### <font color=green> Answer:\n","\n","    "]},{"cell_type":"markdown","metadata":{},"source":["ngram_rangetuple (min_n, max_n), default=(1, 1)\n","The lower and upper boundary of the range of n-values for different word n-grams or char n-grams to be extracted. All values of n such such that min_n <= n <= max_n will be used. For example an ngram_range of (1, 1) means only unigrams, (1, 2) means unigrams and bigrams, and (2, 2) means only bigrams. Only applies if analyzer is not callable."]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":322},"executionInfo":{"elapsed":111,"status":"ok","timestamp":1648585221624,"user":{"displayName":"Claire Z.","userId":"15310629086553932750"},"user_tz":300},"id":"VqH89CB5-OX8","outputId":"1d603891-4193-47a9-e306-e93a8fb265da"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n","  warnings.warn(msg, category=FutureWarning)\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>and</th>\n","      <th>and sky</th>\n","      <th>beautiful</th>\n","      <th>beautiful sky</th>\n","      <th>blue</th>\n","      <th>blue and</th>\n","      <th>blue cheese</th>\n","      <th>cheese</th>\n","      <th>is</th>\n","      <th>is beautiful</th>\n","      <th>...</th>\n","      <th>is so</th>\n","      <th>love</th>\n","      <th>love blue</th>\n","      <th>sky</th>\n","      <th>sky is</th>\n","      <th>so</th>\n","      <th>so blue</th>\n","      <th>the</th>\n","      <th>the beautiful</th>\n","      <th>the sky</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>4 rows × 21 columns</p>\n","</div>"],"text/plain":["   and  and sky  beautiful  beautiful sky  blue  blue and  blue cheese  \\\n","0    0        0          0              0     1         0            0   \n","1    1        1          1              0     1         1            0   \n","2    0        0          1              1     1         0            0   \n","3    0        0          0              0     1         0            1   \n","\n","   cheese  is  is beautiful  ...  is so  love  love blue  sky  sky is  so  \\\n","0       0   1             0  ...      0     0          0    1       1   0   \n","1       0   2             1  ...      0     0          0    2       2   0   \n","2       0   1             0  ...      1     0          0    1       1   1   \n","3       1   0             0  ...      0     1          1    0       0   0   \n","\n","   so blue  the  the beautiful  the sky  \n","0        0    1              0        1  \n","1        0    0              0        0  \n","2        1    1              1        0  \n","3        0    0              0        0  \n","\n","[4 rows x 21 columns]"]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["vectorizer_Bi_Grams = CountVectorizer(max_features=1000, ngram_range=(1, 2))\n","Bi_Grams_matrix = vectorizer_Bi_Grams.fit_transform(corpus).toarray()\n","pd.DataFrame(np.round(Bi_Grams_matrix,2),columns=vectorizer_Bi_Grams.get_feature_names())"]},{"cell_type":"markdown","metadata":{"id":"oNPIe3cL-OX8"},"source":["### <font color=green> End of Answer"]},{"cell_type":"markdown","metadata":{"id":"VM6IRfwJ-OX9"},"source":["### Vectorization with Term Frequency – Inverse Document Frequency (TF-IDF)\n","\n","Now, let's do feature extraction (vectorization) using the TF-IDF approach. <br> <br> See full documentation here: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html#sklearn.feature_extraction.text.TfidfTransformer <br> <br>\n","Import the vectorizer first and define it by specifying the functions (look up the specified parameters in the documentation):"]},{"cell_type":"code","execution_count":25,"metadata":{"id":"NMQS2XcT-OX9"},"outputs":[],"source":["from sklearn.feature_extraction.text import TfidfVectorizer \n","\n","vectorizer_TF_IDF = TfidfVectorizer(norm = None, smooth_idf = True)"]},{"cell_type":"markdown","metadata":{"id":"bSJrSlTc-OX9"},"source":["Let's vectorize our corpus now:"]},{"cell_type":"code","execution_count":26,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":231},"executionInfo":{"elapsed":256,"status":"ok","timestamp":1648585331674,"user":{"displayName":"Claire Z.","userId":"15310629086553932750"},"user_tz":300},"id":"WfqZ6VXe-OX9","outputId":"23bf0c78-1514-4a0a-bf2b-bd82661ec1c3"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n","  warnings.warn(msg, category=FutureWarning)\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>and</th>\n","      <th>beautiful</th>\n","      <th>blue</th>\n","      <th>cheese</th>\n","      <th>is</th>\n","      <th>love</th>\n","      <th>sky</th>\n","      <th>so</th>\n","      <th>the</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>1.0</td>\n","      <td>0.00</td>\n","      <td>1.22</td>\n","      <td>0.00</td>\n","      <td>1.22</td>\n","      <td>0.00</td>\n","      <td>1.51</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1.92</td>\n","      <td>1.51</td>\n","      <td>1.0</td>\n","      <td>0.00</td>\n","      <td>2.45</td>\n","      <td>0.00</td>\n","      <td>2.45</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.00</td>\n","      <td>1.51</td>\n","      <td>1.0</td>\n","      <td>0.00</td>\n","      <td>1.22</td>\n","      <td>0.00</td>\n","      <td>1.22</td>\n","      <td>1.92</td>\n","      <td>1.51</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>1.0</td>\n","      <td>1.92</td>\n","      <td>0.00</td>\n","      <td>1.92</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    and  beautiful  blue  cheese    is  love   sky    so   the\n","0  0.00       0.00   1.0    0.00  1.22  0.00  1.22  0.00  1.51\n","1  1.92       1.51   1.0    0.00  2.45  0.00  2.45  0.00  0.00\n","2  0.00       1.51   1.0    0.00  1.22  0.00  1.22  1.92  1.51\n","3  0.00       0.00   1.0    1.92  0.00  1.92  0.00  0.00  0.00"]},"execution_count":26,"metadata":{},"output_type":"execute_result"}],"source":["TF_IDF_matrix = vectorizer_TF_IDF.fit_transform(corpus).toarray()\n","pd.DataFrame(np.round(TF_IDF_matrix, 2), columns=vectorizer_TF_IDF.get_feature_names())"]},{"cell_type":"markdown","metadata":{"id":"gu4G919s-OX9"},"source":["Have a look at the IDF weights:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":102,"status":"ok","timestamp":1648585374955,"user":{"displayName":"Claire Z.","userId":"15310629086553932750"},"user_tz":300},"id":"_xQwCOJw-OX9","outputId":"1739e353-ecbd-4d69-ea3c-698d1b072e98"},"outputs":[{"name":"stdout","output_type":"stream","text":["[1.92 1.51 1.   1.92 1.22 1.92 1.22 1.92 1.51]\n"]}],"source":["print(np.round(vectorizer_TF_IDF.idf_,2))"]},{"cell_type":"markdown","metadata":{"id":"ICCU3Dli-OX9"},"source":["It's a good idea to normalize the TF-IDF matrix, i.e. restrict all entries to be between 0 and 1. Some text mining models require normalized matrices. Norm parameter is used for this purpose (you can look it up in the documentation):"]},{"cell_type":"code","execution_count":27,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":231},"executionInfo":{"elapsed":125,"status":"ok","timestamp":1648585589102,"user":{"displayName":"Claire Z.","userId":"15310629086553932750"},"user_tz":300},"id":"bz4bFsUG-OX-","outputId":"d5b801b7-a347-4c2b-de63-a1b686b5f2da"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n","  warnings.warn(msg, category=FutureWarning)\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>and</th>\n","      <th>beautiful</th>\n","      <th>blue</th>\n","      <th>cheese</th>\n","      <th>is</th>\n","      <th>love</th>\n","      <th>sky</th>\n","      <th>so</th>\n","      <th>the</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.40</td>\n","      <td>0.00</td>\n","      <td>0.49</td>\n","      <td>0.00</td>\n","      <td>0.49</td>\n","      <td>0.00</td>\n","      <td>0.60</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.44</td>\n","      <td>0.35</td>\n","      <td>0.23</td>\n","      <td>0.00</td>\n","      <td>0.56</td>\n","      <td>0.00</td>\n","      <td>0.56</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.00</td>\n","      <td>0.43</td>\n","      <td>0.29</td>\n","      <td>0.00</td>\n","      <td>0.35</td>\n","      <td>0.00</td>\n","      <td>0.35</td>\n","      <td>0.55</td>\n","      <td>0.43</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.35</td>\n","      <td>0.66</td>\n","      <td>0.00</td>\n","      <td>0.66</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    and  beautiful  blue  cheese    is  love   sky    so   the\n","0  0.00       0.00  0.40    0.00  0.49  0.00  0.49  0.00  0.60\n","1  0.44       0.35  0.23    0.00  0.56  0.00  0.56  0.00  0.00\n","2  0.00       0.43  0.29    0.00  0.35  0.00  0.35  0.55  0.43\n","3  0.00       0.00  0.35    0.66  0.00  0.66  0.00  0.00  0.00"]},"execution_count":27,"metadata":{},"output_type":"execute_result"}],"source":["vectorizer_TF_IDF = TfidfVectorizer(norm = 'l2', smooth_idf = True)\n","TF_IDF_matrix = vectorizer_TF_IDF.fit_transform(corpus).todense()\n","pd.DataFrame(np.round(TF_IDF_matrix,2), columns=vectorizer_TF_IDF.get_feature_names())"]},{"cell_type":"markdown","metadata":{"id":"ItXe9YGf-OX-"},"source":["### **<font color=green> EXERCISE 3: You are given a new small corpus called corpus_exercise (see below). Your ultimate task is to normalize (pre-process) the corpus and produce the TF-IDF and the Bag-of-Words representations of the data. Follow the steps below to complete this exercise:</font>**"]},{"cell_type":"markdown","metadata":{"id":"TX7snDM0-OX-"},"source":["Step 1. Download a file Text_Normalization_Function.ipynb from Canvas and put it into the same directory(!) as the current Jupyter notebook. That file defines a relatively sophisticated text normalization function. (OPTIONAL: you can explore what that file does when you are done with this exercise.)"]},{"cell_type":"markdown","metadata":{"id":"bo2LYLCn-OX-"},"source":["Step 2. Run the file Text_Normalization_Function.ipynb to define the text normalization function:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7gGPNa2G-OX-"},"outputs":[],"source":["%run ./Text_Normalization_Function.ipynb"]},{"cell_type":"markdown","metadata":{"id":"SluD2Ev--OX-"},"source":["Step 3. Define the corpus_exercise text corpus:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DJeshMYw-OX-"},"outputs":[],"source":["corpus_exercise = ['python is great for text mining',\n","          'anyone can learn python and do text mining', \n","          'python can go without eating for days',\n","          'python can be a great pet']"]},{"cell_type":"markdown","metadata":{"id":"5Z9c4gLK-OX_"},"source":["Step 4. Normalize the corpus_exercise text corpus and call its normalized version NORM_corpus:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":93,"status":"ok","timestamp":1648585705191,"user":{"displayName":"Claire Z.","userId":"15310629086553932750"},"user_tz":300},"id":"edbM-_Hj-OX_","outputId":"0b412a0d-85e5-42fa-ba55-9c365640c410"},"outputs":[{"data":{"text/plain":["['python great text mining',\n"," 'anyone learn python text mining',\n"," 'python without eat day',\n"," 'python great pet']"]},"execution_count":38,"metadata":{},"output_type":"execute_result"}],"source":["NORM_corpus = normalize_corpus(corpus_exercise)\n","NORM_corpus"]},{"cell_type":"markdown","metadata":{"id":"EVuG8bXj-OX_"},"source":["Step 5. Compute and print out the TF-IDF and the Bag-of-Words representations for NORM_corpus (WRITE the lines of code needed in the cell below):"]},{"cell_type":"markdown","metadata":{"id":"mz_PqkBm-OX_"},"source":["### <font color=green> Answer for E3:"]},{"cell_type":"markdown","metadata":{"id":"dCEpmlj0-OX_"},"source":["The bag-of-words representation of the normalized corpus (NORM_corpus):"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":175},"executionInfo":{"elapsed":106,"status":"ok","timestamp":1648585923736,"user":{"displayName":"Claire Z.","userId":"15310629086553932750"},"user_tz":300},"id":"iDlSzq61-OX_","outputId":"a3df3e9b-c1bb-4f36-cc4b-c04815590da0"},"outputs":[{"data":{"text/html":["\n","  <div id=\"df-b52122e5-8386-48a6-a7f1-4f135afe33f3\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>anyone</th>\n","      <th>day</th>\n","      <th>eat</th>\n","      <th>great</th>\n","      <th>learn</th>\n","      <th>mining</th>\n","      <th>pet</th>\n","      <th>python</th>\n","      <th>text</th>\n","      <th>without</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b52122e5-8386-48a6-a7f1-4f135afe33f3')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-b52122e5-8386-48a6-a7f1-4f135afe33f3 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-b52122e5-8386-48a6-a7f1-4f135afe33f3');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["   anyone  day  eat  great  learn  mining  pet  python  text  without\n","0       0    0    0      1      0       1    0       1     1        0\n","1       1    0    0      0      1       1    0       1     1        0\n","2       0    1    1      0      0       0    0       1     0        1\n","3       0    0    0      1      0       0    1       1     0        0"]},"execution_count":39,"metadata":{},"output_type":"execute_result"}],"source":["vectorizer_BOW = CountVectorizer(max_features=1000) #BOW = bag-of-words\n","BOW_matrix = vectorizer_BOW.fit_transform(NORM_corpus).toarray()\n","pd.DataFrame(np.round(BOW_matrix,2),columns=vectorizer_BOW.get_feature_names())"]},{"cell_type":"markdown","metadata":{"id":"Mz2W_hiB-OYA"},"source":["The TF-IDF representation of the corpus:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":175},"executionInfo":{"elapsed":118,"status":"ok","timestamp":1648586126752,"user":{"displayName":"Claire Z.","userId":"15310629086553932750"},"user_tz":300},"id":"zYvqy9B--OYA","outputId":"a7819a43-9191-4f63-db44-45af0d3aa89a"},"outputs":[{"data":{"text/html":["\n","  <div id=\"df-e0e31aa0-6ee2-4789-b1f2-0156b398890e\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>anyone</th>\n","      <th>day</th>\n","      <th>eat</th>\n","      <th>great</th>\n","      <th>learn</th>\n","      <th>mining</th>\n","      <th>pet</th>\n","      <th>python</th>\n","      <th>text</th>\n","      <th>without</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.54</td>\n","      <td>0.00</td>\n","      <td>0.54</td>\n","      <td>0.00</td>\n","      <td>0.36</td>\n","      <td>0.54</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.53</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.53</td>\n","      <td>0.42</td>\n","      <td>0.00</td>\n","      <td>0.28</td>\n","      <td>0.42</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.00</td>\n","      <td>0.55</td>\n","      <td>0.55</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.29</td>\n","      <td>0.00</td>\n","      <td>0.55</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.57</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.73</td>\n","      <td>0.38</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e0e31aa0-6ee2-4789-b1f2-0156b398890e')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-e0e31aa0-6ee2-4789-b1f2-0156b398890e button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-e0e31aa0-6ee2-4789-b1f2-0156b398890e');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["   anyone   day   eat  great  learn  mining   pet  python  text  without\n","0    0.00  0.00  0.00   0.54   0.00    0.54  0.00    0.36  0.54     0.00\n","1    0.53  0.00  0.00   0.00   0.53    0.42  0.00    0.28  0.42     0.00\n","2    0.00  0.55  0.55   0.00   0.00    0.00  0.00    0.29  0.00     0.55\n","3    0.00  0.00  0.00   0.57   0.00    0.00  0.73    0.38  0.00     0.00"]},"execution_count":41,"metadata":{},"output_type":"execute_result"}],"source":["vectorizer_TF_IDF = TfidfVectorizer(norm = 'l2', smooth_idf = True)\n","TF_IDF_matrix = vectorizer_TF_IDF.fit_transform(NORM_corpus).todense()\n","pd.DataFrame(np.round(TF_IDF_matrix,2), columns=vectorizer_TF_IDF.get_feature_names())"]},{"cell_type":"markdown","metadata":{"id":"ijqHSdF5-OYA"},"source":["### <font color=green> End of Answer"]},{"cell_type":"markdown","metadata":{"id":"1pFFK0v_-OYA"},"source":["### **<font color=green> OPTIONAL EXERCISE 4: Explore the Text_Normalization_Function.ipynb notebook that defines a text normalization function. The file is available on Canvas in the Lab 2 Assignment (no answer is needed). </font>**"]}],"metadata":{"colab":{"collapsed_sections":["cOtUSOgk-OX0","b7DEot9N-OX5","io0YSTri-OX8","oNPIe3cL-OX8","ijqHSdF5-OYA"],"name":"Lab_2_Normalization_and_Vectorization_S2022.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3.9.9 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.9"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false},"vscode":{"interpreter":{"hash":"aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"}}},"nbformat":4,"nbformat_minor":0}
