{"cells":[{"cell_type":"markdown","metadata":{"id":"rfc7MJHKs7Fg"},"source":["# **LAB 3. TEXT CLASSIFICATION** \n","\n","### **<font color=green>INSTRUCTIONS:</font>** <br> \n","\n","**<font color=green> 1. Look for EXERCISES in the script.</font>** <br>\n","\n","**<font color=green> 2. Each student INDIVIDUALLY uploads this script with their answers embedded to Canvas by the end of the day on Wednesday.</font>** "]},{"cell_type":"markdown","metadata":{"id":"682gGDBss7Fi"},"source":["### **Lab Objective**\n","Our objective is to classify consumer messages based on the topic of the message. Today, you will:<br>\n","1. Learn how to vectorize *training* data and *testing* data (**watch out for very important nuances in the script!**)\n","2. Do feature selection using **filter methods**: chi-squared statistics and a variation of the entropy method (mutual information)\n","3. Train and test a **Naive Bayes classifier** for text data (you can use other methods, such as SVM, etc.)\n","\n","\n","### **Session Prep**\n","Install the modules we'll need:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14630,"status":"ok","timestamp":1649186717314,"user":{"displayName":"Claire Z.","userId":"15310629086553932750"},"user_tz":300},"id":"CqhOSNCFs7Fj","outputId":"9d534b9e-05da-4668-8207-37cadbe9e83e","trusted":true},"outputs":[],"source":["import sys\n","\n","#!{sys.executable} -m pip install numpy\n","import numpy as np\n","\n","#!{sys.executable} -m pip install sklearn\n","from sklearn import metrics\n","\n","#!{sys.executable} -m pip install pandas\n","import pandas as pd\n","\n","#!{sys.executable} -m pip install nltk\n","import nltk\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n","\n","#!{sys.executable} -m pip install nbformat\n","!{sys.executable} -m pip install pattern3"]},{"cell_type":"markdown","metadata":{"id":"ZAC2GNQcs7Fl"},"source":["Next, we will define the text normalization function. The function is defined in a separate file called Text_Normalization_Function.ipynb (we used in Lab 2 as well). \n","\n","---\n","**IMPORTANT**: \n","\n","1.   If you work locally on your own machine: make sure Text_Normalization_Function.ipynb file is in the same directory as the current notebook\n","2.   If on Google Colab: upload Text_Normalization_Function.ipynb file to session storage\n","\n","---\n","\n","Let's execute the file:\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UBBvOkqus7Fl","trusted":true},"outputs":[],"source":["%run ./Text_Normalization_Function.ipynb  #defining text normalization function"]},{"cell_type":"markdown","metadata":{"id":"WAusnosGs7Fl"},"source":["### **Downloading and Exploring Data**\n","Download the dataset from the dataset sklearn's collection of datasets (sklearn.datasets). The dataset we need is called fetch_20newsgroups:<br><br>\n","*Note: You can check out the documentation for the dataset here: https://bit.ly/3aM5tUo*"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":351,"status":"ok","timestamp":1649186805587,"user":{"displayName":"Claire Z.","userId":"15310629086553932750"},"user_tz":300},"id":"C-6WmiSRs7Fm","trusted":true},"outputs":[],"source":["from sklearn.datasets import fetch_20newsgroups\n","warnings.filterwarnings(\"ignore\", category=DeprecationWarning) #supress deprication warnings if any"]},{"cell_type":"markdown","metadata":{"id":"32Y0d_90s7Fm"},"source":["Out of 20 newsgroups (topics) available, we will use posts on **4** topics: atheism, religion, computer graphics, and science. You can refer to those newsgroups as **classes** or **categories**. Note right away, that the \"atheism\" and \"religion\" classes are likely **similar** to each other: people might be using similar words when they talk about atheism and religion."]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":194,"status":"ok","timestamp":1649186999009,"user":{"displayName":"Claire Z.","userId":"15310629086553932750"},"user_tz":300},"id":"S2SK_3vNs7Fn","trusted":true},"outputs":[],"source":["categories = ['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']"]},{"cell_type":"markdown","metadata":{"id":"RHJoKiz8s7Fn"},"source":["Now, let's create our **training** and **testing** datasets. We do that by picking the posts belonging to the selected classes, marked in the dataset \"test\" or \"train\". We remove headers (likely, email or letter headers), footers (containing author's signatures, etc.), and quotes: "]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":11680,"status":"ok","timestamp":1649187691749,"user":{"displayName":"Claire Z.","userId":"15310629086553932750"},"user_tz":300},"id":"mNMm7Dmos7Fo","trusted":true},"outputs":[],"source":["corpus_train = fetch_20newsgroups(categories = categories,\n","                                  subset = 'train', \n","                                  remove = ('headers', 'footers', 'quotes')) \n","\n","corpus_test = fetch_20newsgroups(categories = categories,\n","                                 subset='test', \n","                                 remove=('headers', 'footers', 'quotes')) "]},{"cell_type":"markdown","metadata":{"id":"BHs_6sYIs7Fo"},"source":["Let's inspect the data by looking at the training text corpus. First, have a look at one of the posts:"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":294,"status":"ok","timestamp":1649187704550,"user":{"displayName":"Claire Z.","userId":"15310629086553932750"},"user_tz":300},"id":"ijD9EWd8s7Fo","outputId":"8a3b4a37-7b95-4499-b500-4ea91ad6858b","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Acorn Replay running on a 25MHz ARM 3 processor (the ARM 3 is about 20% slower\n","than the ARM 6) does this in software (off a standard CD-ROM). 16 bit colour at\n","about the same resolution (so what if the computer only has 8 bit colour\n","support, real-time dithering too...). The 3D0/O is supposed to have a couple of\n","DSPs - the ARM being used for housekeeping.\n","\n","\n","A 25MHz ARM 6xx should clock around 20 ARM MIPS, say 18 flat out. Depends\n","really on the surrounding system and whether you are talking ARM6x or ARM6xx\n","(the latter has a cache, and so is essential to run at this kind of speed with\n","slower memory).\n","\n","I'll stop saying things there 'cos I'll hopefully be working for ARM after\n","graduation...\n","\n","Mike\n","\n","PS Don't pay heed to what reps from Philips say; if the 3D0/O doesn't beat the\n","   pants off 3DI then I'll eat this postscript.\n"]}],"source":["print(corpus_train.data[7])       "]},{"cell_type":"markdown","metadata":{"id":"FD5PSKzls7Fp"},"source":["The class labels for each message that you will be using for training and testing are encoded as numbers and can be accessed via attribute **.target** and their names can be accessed via attribute **.target_names**:"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":329,"status":"ok","timestamp":1649187813297,"user":{"displayName":"Claire Z.","userId":"15310629086553932750"},"user_tz":300},"id":"lvrByrt6s7Fp","outputId":"e7675116-7b93-4cd5-c528-b36b38ffbe33","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Category names:  ['alt.atheism', 'comp.graphics', 'sci.space', 'talk.religion.misc']\n","Categories for first 10 observations:  [1 3 2 0 2 0 2 1 2 1]\n","Number of posts in the training dataset:  2034\n"]}],"source":["print(\"Category names: \", corpus_train.target_names)    \n","print(\"Categories for first 10 observations: \", corpus_train.target[:10])     \n","print(\"Number of posts in the training dataset: \", corpus_train.filenames.shape[0]) "]},{"cell_type":"markdown","metadata":{"id":"YaNpjWK6s7Fp"},"source":["Let's now define a function and call it **fmat_descr_fun** to be used later to describe the feature matrix (vectorized corpus). The function prints out the dimensions of the matrix, share of non-zero elements and so on. The function will take two inputs: the feature matrix and your vectorizer function. In what follows, you can see the descriptives that the function provides:"]},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":199,"status":"ok","timestamp":1649187825154,"user":{"displayName":"Claire Z.","userId":"15310629086553932750"},"user_tz":300},"id":"It9pVDAds7Fq","trusted":true},"outputs":[],"source":["def fmat_descr_fun(your_feature_matrix,your_vectorizer):\n","    print(\"Dimensions (number of posts, number of features): \", your_feature_matrix.shape)  \n","    print(\"The first 5 features - names: \", your_vectorizer.get_feature_names()[0:5]) \n","    print(\"Share of non-zero elements in the matrix: \", \n","          your_feature_matrix.nnz / (float(your_feature_matrix.shape[0]) * float(your_feature_matrix.shape[1]))) #nnz: Get the count of explicitly-stored values (nonzeros)\n","    print(\"Average number of features present, per post: \", \n","          round(your_feature_matrix.nnz/float(your_feature_matrix.shape[0]),1))"]},{"cell_type":"markdown","metadata":{"id":"WeBfKdw2s7Fq"},"source":["### **Feature Extraction for TRAINING Data**\n","\n","Let's do feature extraction for our **TRAINING** data using the **\"Bag-of-words\"** method and **TF-IDF** method (optional).\n","<br><br>\n","\n","### **\"Bag-of-words\" Vectorization for TRAINING data**\n","\n","As you remember from Lab 2, to do the Bag-of-Words vectorization, we can use the **CountVectorizer()** function from the sklearn package: "]},{"cell_type":"code","execution_count":18,"metadata":{"executionInfo":{"elapsed":227,"status":"ok","timestamp":1649187850733,"user":{"displayName":"Claire Z.","userId":"15310629086553932750"},"user_tz":300},"id":"_JTvN--vs7Fq","trusted":true},"outputs":[],"source":["from sklearn.feature_extraction.text import CountVectorizer"]},{"cell_type":"markdown","metadata":{"id":"l3mBLF8Ns7Fq"},"source":["Let's define our Bag-of-Words vectorizer:"]},{"cell_type":"code","execution_count":19,"metadata":{"executionInfo":{"elapsed":191,"status":"ok","timestamp":1649188439500,"user":{"displayName":"Claire Z.","userId":"15310629086553932750"},"user_tz":300},"id":"TNAM7tE2s7Fq","trusted":true},"outputs":[],"source":["bow_vectorizer = CountVectorizer()"]},{"cell_type":"markdown","metadata":{"id":"apGosl13s7Fr"},"source":["Now we can apply the vectorizer we defined, bow_vectorizer, to our training dataset, **without normalizing** it first (though tokenization will be done by the vectorizer). Remember, to create the corpus vocabulary and to vectorize the data accroding to that vocabulary, we use the **.fit_transform** method:"]},{"cell_type":"code","execution_count":20,"metadata":{"executionInfo":{"elapsed":585,"status":"ok","timestamp":1649188441162,"user":{"displayName":"Claire Z.","userId":"15310629086553932750"},"user_tz":300},"id":"cbed64gGs7Fr","trusted":true},"outputs":[],"source":["corpus_train_bow = bow_vectorizer.fit_transform(corpus_train.data)"]},{"cell_type":"markdown","metadata":{"id":"KwPTCAYGs7Fr"},"source":["Describe the Bag-of-Words matrix that you got by applying the function **fmat_descr_fun** we defined above:"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":528,"status":"ok","timestamp":1649188442950,"user":{"displayName":"Claire Z.","userId":"15310629086553932750"},"user_tz":300},"id":"yM2Ffnv7s7Fr","outputId":"7abaa8ea-6402-4b60-95ae-0d69d402ae82","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Dimensions (number of posts, number of features):  (2034, 26879)\n","The first 5 features - names:  ['00', '000', '0000', '00000', '000000']\n","Share of non-zero elements in the matrix:  0.0035978272269590263\n","Average number of features present, per post:  96.7\n"]}],"source":["fmat_descr_fun(corpus_train_bow, bow_vectorizer)"]},{"cell_type":"markdown","metadata":{"id":"w4FjKOFIs7Fr"},"source":["Let's have a look at the first 5 rows in the resulting matrix:"]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":487},"executionInfo":{"elapsed":229,"status":"ok","timestamp":1649188444405,"user":{"displayName":"Claire Z.","userId":"15310629086553932750"},"user_tz":300},"id":"vsIF5nZts7Fr","outputId":"96573485-1829-43ec-815d-b6235aa585c1","trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>00</th>\n","      <th>000</th>\n","      <th>0000</th>\n","      <th>00000</th>\n","      <th>000000</th>\n","      <th>000005102000</th>\n","      <th>000062david42</th>\n","      <th>0001</th>\n","      <th>000100255pixel</th>\n","      <th>00041032</th>\n","      <th>...</th>\n","      <th>zurich</th>\n","      <th>zurvanism</th>\n","      <th>zus</th>\n","      <th>zvi</th>\n","      <th>zwaartepunten</th>\n","      <th>zwak</th>\n","      <th>zwakke</th>\n","      <th>zware</th>\n","      <th>zwarte</th>\n","      <th>zyxel</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>2029</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2030</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2031</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2032</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2033</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>2034 rows × 26879 columns</p>\n","</div>"],"text/plain":["      00  000  0000  00000  000000  000005102000  000062david42  0001  \\\n","0      0    0     0      0       0             0              0     0   \n","1      0    0     0      0       0             0              0     0   \n","2      0    0     0      0       0             0              0     0   \n","3      0    0     0      0       0             0              0     0   \n","4      0    0     0      0       0             0              0     0   \n","...   ..  ...   ...    ...     ...           ...            ...   ...   \n","2029   0    0     0      0       0             0              0     0   \n","2030   0    0     0      0       0             0              0     0   \n","2031   0    0     0      0       0             0              0     0   \n","2032   0    0     0      0       0             0              0     0   \n","2033   0    0     0      0       0             0              0     0   \n","\n","      000100255pixel  00041032  ...  zurich  zurvanism  zus  zvi  \\\n","0                  0         0  ...       0          0    0    0   \n","1                  0         0  ...       0          0    0    0   \n","2                  0         0  ...       0          0    0    0   \n","3                  0         0  ...       0          0    0    0   \n","4                  0         0  ...       0          0    0    0   \n","...              ...       ...  ...     ...        ...  ...  ...   \n","2029               0         0  ...       0          0    0    0   \n","2030               0         0  ...       0          0    0    0   \n","2031               0         0  ...       0          0    0    0   \n","2032               0         0  ...       0          0    0    0   \n","2033               0         0  ...       0          0    0    0   \n","\n","      zwaartepunten  zwak  zwakke  zware  zwarte  zyxel  \n","0                 0     0       0      0       0      0  \n","1                 0     0       0      0       0      0  \n","2                 0     0       0      0       0      0  \n","3                 0     0       0      0       0      0  \n","4                 0     0       0      0       0      0  \n","...             ...   ...     ...    ...     ...    ...  \n","2029              0     0       0      0       0      0  \n","2030              0     0       0      0       0      0  \n","2031              0     0       0      0       0      0  \n","2032              0     0       0      0       0      0  \n","2033              0     0       0      0       0      0  \n","\n","[2034 rows x 26879 columns]"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["#convert vectorized data to a dataframe and give columns their names\n","corpus_train_bow_table = pd.DataFrame(data = corpus_train_bow.todense(), columns = bow_vectorizer.get_feature_names())\n","corpus_train_bow_table"]},{"cell_type":"markdown","metadata":{"id":"WIK3Q5Wqs7Fs"},"source":["### **<font color=green>EXERCISE 1:</font>**\n","**<font color=green> Answer the following questions using the descriptives of the Bag-of-Words matrix:</font>** <br><br>\n","**<font color=green>1.1. How many features does the Bag-of-Words matrix contain?</font>** <br><br> \n","Your answer:"]},{"cell_type":"markdown","metadata":{"id":"igx25XjKs7Fs"},"source":["26879"]},{"cell_type":"markdown","metadata":{"id":"X90LjiY4s7Fs"},"source":["**<font color=green>1.2. Is the Bag-of-Words matrix sparse? Explain your answer: </font>** <br><br> \n","Your answer:"]},{"cell_type":"markdown","metadata":{"id":"wn0B8iFhs7Fs"},"source":["Yes. Because there are many 0 in the matrix."]},{"cell_type":"markdown","metadata":{"id":"O9A5fTmrs7Fs"},"source":["Now, let's normalize our training data and call the normalized corpus **NORM_corpus_train** and then vectorize it using the same Bag-of-Words approach to vectorization (note that it will take some time for the normalization function to finish its job):"]},{"cell_type":"code","execution_count":24,"metadata":{"executionInfo":{"elapsed":37002,"status":"ok","timestamp":1649188032300,"user":{"displayName":"Claire Z.","userId":"15310629086553932750"},"user_tz":300},"id":"c4IUPXu6s7Fs","trusted":true},"outputs":[],"source":["NORM_corpus_train = normalize_corpus(corpus_train.data)"]},{"cell_type":"markdown","metadata":{"id":"0BGuTWc9s7Ft"},"source":["### **<font color=green>EXERCISE 2:</font>**\n","**<font color=green>Vectorize the normalized training corpus using the Bag-of-Words approach and name the vectorized corpus NORM_corpus_train_bow. Describe the normalized training corpus using the fmat_descr_fun function. <br><br> 2.1. What differences do you see between normalized and non-normalized feature matrices?</font>** <br><br>\n","Your answer:"]},{"cell_type":"markdown","metadata":{"id":"U8QUIzqgSz2i"},"source":["* The dimention of the matrix has been reduced. There are less columns in the new matrix. After normalization, the matrix has 21090 columns while the old matrix has 26879 columns.\n","\n","* Share of non-zero elements in the matrix has been reduced as well.\n","\n","* Average number of features present, per post has been smaller, showing that the features in the new matrix are more discriminating for classification."]},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":560},"executionInfo":{"elapsed":561,"status":"ok","timestamp":1649189183003,"user":{"displayName":"Claire Z.","userId":"15310629086553932750"},"user_tz":300},"id":"paG03LaWRjLG","outputId":"516f4450-add3-4921-cf4b-1086bde396ee"},"outputs":[{"name":"stdout","output_type":"stream","text":["Dimensions (number of posts, number of features):  (2034, 21061)\n","The first 5 features - names:  ['000062david42', '000100255pixel', '000usd', '001200201pixel', '00index']\n","Share of non-zero elements in the matrix:  0.002958676433492318\n","Average number of features present, per post:  62.3\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>000062david42</th>\n","      <th>000100255pixel</th>\n","      <th>000usd</th>\n","      <th>001200201pixel</th>\n","      <th>00index</th>\n","      <th>00pm</th>\n","      <th>01a</th>\n","      <th>023b</th>\n","      <th>04g</th>\n","      <th>054589e</th>\n","      <th>...</th>\n","      <th>zurich</th>\n","      <th>zurvanism</th>\n","      <th>zus</th>\n","      <th>zvi</th>\n","      <th>zwaartepunten</th>\n","      <th>zwak</th>\n","      <th>zwakke</th>\n","      <th>zware</th>\n","      <th>zwarte</th>\n","      <th>zyxel</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>2029</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2030</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2031</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2032</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2033</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>2034 rows × 21061 columns</p>\n","</div>"],"text/plain":["      000062david42  000100255pixel  000usd  001200201pixel  00index  00pm  \\\n","0                 0               0       0               0        0     0   \n","1                 0               0       0               0        0     0   \n","2                 0               0       0               0        0     0   \n","3                 0               0       0               0        0     0   \n","4                 0               0       0               0        0     0   \n","...             ...             ...     ...             ...      ...   ...   \n","2029              0               0       0               0        0     0   \n","2030              0               0       0               0        0     0   \n","2031              0               0       0               0        0     0   \n","2032              0               0       0               0        0     0   \n","2033              0               0       0               0        0     0   \n","\n","      01a  023b  04g  054589e  ...  zurich  zurvanism  zus  zvi  \\\n","0       0     0    0        0  ...       0          0    0    0   \n","1       0     0    0        0  ...       0          0    0    0   \n","2       0     0    0        0  ...       0          0    0    0   \n","3       0     0    0        0  ...       0          0    0    0   \n","4       0     0    0        0  ...       0          0    0    0   \n","...   ...   ...  ...      ...  ...     ...        ...  ...  ...   \n","2029    0     0    0        0  ...       0          0    0    0   \n","2030    0     0    0        0  ...       0          0    0    0   \n","2031    0     0    0        0  ...       0          0    0    0   \n","2032    0     0    0        0  ...       0          0    0    0   \n","2033    0     0    0        0  ...       0          0    0    0   \n","\n","      zwaartepunten  zwak  zwakke  zware  zwarte  zyxel  \n","0                 0     0       0      0       0      0  \n","1                 0     0       0      0       0      0  \n","2                 0     0       0      0       0      0  \n","3                 0     0       0      0       0      0  \n","4                 0     0       0      0       0      0  \n","...             ...   ...     ...    ...     ...    ...  \n","2029              0     0       0      0       0      0  \n","2030              0     0       0      0       0      0  \n","2031              0     0       0      0       0      0  \n","2032              0     0       0      0       0      0  \n","2033              0     0       0      0       0      0  \n","\n","[2034 rows x 21061 columns]"]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":["bow_vectorizer = CountVectorizer()\n","NORM_corpus_train_bow = bow_vectorizer.fit_transform(NORM_corpus_train)\n","fmat_descr_fun(NORM_corpus_train_bow, bow_vectorizer)\n","corpus_train_bow_table = pd.DataFrame(data = NORM_corpus_train_bow.todense(), columns = bow_vectorizer.get_feature_names())\n","corpus_train_bow_table"]},{"cell_type":"markdown","metadata":{"id":"WaGJgbAIvUjl"},"source":[]},{"cell_type":"markdown","metadata":{"id":"nWPuVcpVs7Ft"},"source":["## **Feature Selection for TRAINING Data** \n","Let's select the best features for your normalized training corpus, **NORM_corpus_train_bow**, using the **chi-squared statistic** and **mutial information (MI)**, whcih is based on the ideas of **entropy**.\n","\n","---\n","\n","**IMPORTANT** <br> You need to be done with the previous EXERCISE to be able to continue. \n","\n","---\n","\n","We need to import the feature selection function **SelectKBest** first. This function selects k best features based on the results of a test (in our case, we will use chi-squared and mutual information). Also, we need the **chi2** function and **mutual_info_classif** functions:"]},{"cell_type":"code","execution_count":26,"metadata":{"executionInfo":{"elapsed":208,"status":"ok","timestamp":1649188687154,"user":{"displayName":"Claire Z.","userId":"15310629086553932750"},"user_tz":300},"id":"etn0UROZs7Fu","trusted":true},"outputs":[],"source":["from sklearn.feature_selection import SelectKBest, chi2, mutual_info_classif"]},{"cell_type":"markdown","metadata":{"id":"rko6kp5ys7Fu"},"source":["First, we need to specify that we will use SelectKBest function with the chi-squared statistic for feature selection (by setting parameter \"score_func\") and indicate the number of best features we want to find (by setting parameter \"k\"). We'll select 10,000 best features, so **k = 10,000**:<br><br>\n","*Look up the documentation for SelectKBest if needed: https://bit.ly/2Re54Ch*"]},{"cell_type":"markdown","metadata":{},"source":["### Chi-squared statistic"]},{"cell_type":"code","execution_count":27,"metadata":{"executionInfo":{"elapsed":229,"status":"ok","timestamp":1649188689949,"user":{"displayName":"Claire Z.","userId":"15310629086553932750"},"user_tz":300},"id":"8jMWmists7Fu","trusted":true},"outputs":[],"source":["chi2_kbest = SelectKBest(score_func = chi2, k = 10000)"]},{"cell_type":"markdown","metadata":{"id":"PlVOUPLXs7Fu"},"source":["Now, let's find the best features. To do that, we call the **fit_transform** method with our defined kbest function. The fit_transform method takes **2 inputs**: the vectorized representation of the data (NORM_corpus_train_bow) and the array of class labels (corpus_train.target). To see the chi-squared scores for the features use method **scores_**:"]},{"cell_type":"code","execution_count":28,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":200,"status":"ok","timestamp":1649189193087,"user":{"displayName":"Claire Z.","userId":"15310629086553932750"},"user_tz":300},"id":"KSpHgdiqs7Fu","outputId":"4d21322e-05b5-4e20-bc7d-53d5337751bc","trusted":true},"outputs":[{"data":{"text/plain":["array([2.43001686, 2.48287671, 4.96575342, ..., 2.43001686, 4.86003373,\n","       4.96575342])"]},"execution_count":28,"metadata":{},"output_type":"execute_result"}],"source":["NORM_corpus_train_bow_chi2_BEST = chi2_kbest.fit_transform(NORM_corpus_train_bow, corpus_train.target)\n","chi2_kbest.scores_"]},{"cell_type":"markdown","metadata":{"id":"rhc_sntRs7Fu"},"source":["So, which features are best? The **get_support** method with parameter *indices* set to True will return the indecies of the best k features:"]},{"cell_type":"code","execution_count":31,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":538,"status":"ok","timestamp":1649189204909,"user":{"displayName":"Claire Z.","userId":"15310629086553932750"},"user_tz":300},"id":"FvrTUzR8s7Fv","outputId":"3a891eb1-5aa2-4714-bee2-6cd381a8c6d2","trusted":true},"outputs":[{"data":{"text/plain":["array([    2,     5,     7, ..., 21052, 21059, 21060])"]},"execution_count":31,"metadata":{},"output_type":"execute_result"}],"source":["chi2_best_features_ind = chi2_kbest.get_support(indices=True)\n","chi2_best_features_ind"]},{"cell_type":"markdown","metadata":{"id":"ACwwiTFJs7Fv"},"source":["What are the names of the best features, according to the chi-squared statistics?"]},{"cell_type":"code","execution_count":32,"metadata":{"executionInfo":{"elapsed":204,"status":"ok","timestamp":1649189209492,"user":{"displayName":"Claire Z.","userId":"15310629086553932750"},"user_tz":300},"id":"U_YyWdYas7Fv","trusted":true},"outputs":[],"source":["chi2_best_features_names = np.array(bow_vectorizer.get_feature_names())[chi2_best_features_ind]"]},{"cell_type":"markdown","metadata":{"id":"xCg0JqFrs7Fv"},"source":["Let's have a look at the data vectorized using best features selected using the chi-squared statistics:"]},{"cell_type":"code","execution_count":37,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":299},"executionInfo":{"elapsed":209,"status":"ok","timestamp":1649189212529,"user":{"displayName":"Claire Z.","userId":"15310629086553932750"},"user_tz":300},"id":"NSTh2-Lbs7Fv","outputId":"c7db708e-53ca-4947-a5e5-b6410fc573d6","trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>000usd</th>\n","      <th>00pm</th>\n","      <th>023b</th>\n","      <th>0x</th>\n","      <th>1024x768</th>\n","      <th>10bps</th>\n","      <th>10km</th>\n","      <th>10m</th>\n","      <th>110m</th>\n","      <th>115m</th>\n","      <th>...</th>\n","      <th>zoroaster</th>\n","      <th>zoroastrian</th>\n","      <th>zoroastrianism</th>\n","      <th>zoroastrians</th>\n","      <th>zubin</th>\n","      <th>zuck</th>\n","      <th>zullen</th>\n","      <th>zurvanism</th>\n","      <th>zwarte</th>\n","      <th>zyxel</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 10000 columns</p>\n","</div>"],"text/plain":["   000usd  00pm  023b  0x  1024x768  10bps  10km  10m  110m  115m  ...  \\\n","0       0     0     0   0         0      0     0    0     0     0  ...   \n","1       0     0     0   0         0      0     0    0     0     0  ...   \n","2       0     0     0   0         0      0     0    0     0     0  ...   \n","3       0     0     0   0         0      0     0    0     0     0  ...   \n","4       0     0     0   0         0      0     0    0     0     0  ...   \n","\n","   zoroaster  zoroastrian  zoroastrianism  zoroastrians  zubin  zuck  zullen  \\\n","0          0            0               0             0      0     0       0   \n","1          0            0               0             0      0     0       0   \n","2          0            0               0             0      0     0       0   \n","3          0            0               0             0      0     0       0   \n","4          0            0               0             0      0     0       0   \n","\n","   zurvanism  zwarte  zyxel  \n","0          0       0      0  \n","1          0       0      0  \n","2          0       0      0  \n","3          0       0      0  \n","4          0       0      0  \n","\n","[5 rows x 10000 columns]"]},"execution_count":37,"metadata":{},"output_type":"execute_result"}],"source":["#convert vectorized data to a dataframe and give columns their names\n","X_train_bow_chi2_BEST_table = pd.DataFrame(data = NORM_corpus_train_bow_chi2_BEST.todense(), columns = chi2_best_features_names)\n","X_train_bow_chi2_BEST_table.head(5)"]},{"cell_type":"markdown","metadata":{},"source":["### MI"]},{"cell_type":"markdown","metadata":{"id":"8DntEWSQs7Fv"},"source":["### **<font color=green>EXERCISE 3:</font>**\n","**<font color=green>3.1. Select best features using the mutual information statistic. Follow the steps we used for the chi-squared statistic. The first line, showing how to specify the SelectKBest function using mutual information, is provided. You need to complete the script.</font>** <br><br>\n","**<font color=green>3.2. Do mutual information approach and chi-squared statistic select the same best features?</font>** \n","\n","*Note: you can check out the documentation for mutual information function for categorical data here: https://bit.ly/2JGgeeT* <br><br>\n","Your answer (you need to add more lines of Python code to the cell below):"]},{"cell_type":"markdown","metadata":{"id":"OjDmxgsbbkcc"},"source":["They don't have the same best features but their 85.86% of features are the same."]},{"cell_type":"code","execution_count":38,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":299},"executionInfo":{"elapsed":33437,"status":"ok","timestamp":1649189810995,"user":{"displayName":"Claire Z.","userId":"15310629086553932750"},"user_tz":300},"id":"ege2fxm5s7Fw","outputId":"547146fa-db4f-4535-a01c-c2046b439349","trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>023b</th>\n","      <th>0x</th>\n","      <th>1024x768</th>\n","      <th>10km</th>\n","      <th>10m</th>\n","      <th>13h</th>\n","      <th>15m</th>\n","      <th>15rpm</th>\n","      <th>17th</th>\n","      <th>18084tm</th>\n","      <th>...</th>\n","      <th>zorastrian</th>\n","      <th>zoro</th>\n","      <th>zoroaster</th>\n","      <th>zoroastrian</th>\n","      <th>zoroastrianism</th>\n","      <th>zoroastrians</th>\n","      <th>zubin</th>\n","      <th>zuck</th>\n","      <th>zurvanism</th>\n","      <th>zyxel</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 10000 columns</p>\n","</div>"],"text/plain":["   023b  0x  1024x768  10km  10m  13h  15m  15rpm  17th  18084tm  ...  \\\n","0     0   0         0     0    0    0    0      0     0        0  ...   \n","1     0   0         0     0    0    0    0      0     0        0  ...   \n","2     0   0         0     0    0    0    0      0     0        0  ...   \n","3     0   0         0     0    0    0    0      0     0        0  ...   \n","4     0   0         0     0    0    0    0      0     0        0  ...   \n","\n","   zorastrian  zoro  zoroaster  zoroastrian  zoroastrianism  zoroastrians  \\\n","0           0     0          0            0               0             0   \n","1           0     0          0            0               0             0   \n","2           0     0          0            0               0             0   \n","3           0     0          0            0               0             0   \n","4           0     0          0            0               0             0   \n","\n","   zubin  zuck  zurvanism  zyxel  \n","0      0     0          0      0  \n","1      0     0          0      0  \n","2      0     0          0      0  \n","3      0     0          0      0  \n","4      0     0          0      0  \n","\n","[5 rows x 10000 columns]"]},"execution_count":38,"metadata":{},"output_type":"execute_result"}],"source":["MI_kbest = SelectKBest(score_func = mutual_info_classif, k = 10000)\n","NORM_corpus_train_bow_mutual_BEST = MI_kbest.fit_transform(NORM_corpus_train_bow, corpus_train.target)\n","mutual_best_features_ind = MI_kbest.get_support(indices=True)\n","mutual_best_features_names = np.array(bow_vectorizer.get_feature_names())[mutual_best_features_ind]\n","X_train_bow_mutual_BEST_table = pd.DataFrame(data = NORM_corpus_train_bow_mutual_BEST.todense(), columns = mutual_best_features_names)\n","X_train_bow_mutual_BEST_table.head(5)"]},{"cell_type":"code","execution_count":39,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":515,"status":"ok","timestamp":1649190737578,"user":{"displayName":"Claire Z.","userId":"15310629086553932750"},"user_tz":300},"id":"jzZG2AjTZ2l4","outputId":"8ae0146c-07d2-4307-bf11-6fe7198a41de"},"outputs":[{"data":{"text/plain":["0.8592"]},"execution_count":39,"metadata":{},"output_type":"execute_result"}],"source":["def intersection(lst1, lst2):\n","    lst3 = [value for value in lst1 if value in lst2]\n","    return lst3\n","  \n","len(intersection(chi2_best_features_ind,mutual_best_features_ind))/10000"]},{"cell_type":"markdown","metadata":{"id":"aZj5aSwcXCjo"},"source":[]},{"cell_type":"markdown","metadata":{"id":"SjUhCYEGs7Fy"},"source":["### **Feature Extraction for TEST Data**"]},{"cell_type":"markdown","metadata":{"id":"roXI_FZos7Fy"},"source":["Let's normalize and vectorize the **TEST** text corpus (corpus_test.data) using the Bag-of-Words method. <br><br>First, we normalize the testing corpus and call it NORM_corpus_test:"]},{"cell_type":"code","execution_count":40,"metadata":{"executionInfo":{"elapsed":29212,"status":"ok","timestamp":1649189274049,"user":{"displayName":"Claire Z.","userId":"15310629086553932750"},"user_tz":300},"id":"6AOcSXMbs7Fy","trusted":true},"outputs":[],"source":["NORM_corpus_test = normalize_corpus(corpus_test.data)"]},{"cell_type":"markdown","metadata":{"id":"pRdQ7f09s7Fy"},"source":["Now let's vectorize the normalized test corpus, NORM_corpus_test. <br><br>\n","\n","\n","---\n","\n","\n","**IMPORTANT**<br>\n","For transforming test data, you'll use features extracted from the training corpus [You do NOT want to create new feature based on your test data]. <br>Therefore: <br> 1) do **not** define a new vectorizer, use the one used on training data <br>2) use method **.transform** (not .fit_trandform) with your vectorizer to vectorize the test data.\n","\n","\n","---\n","\n"]},{"cell_type":"code","execution_count":41,"metadata":{"executionInfo":{"elapsed":22,"status":"ok","timestamp":1649189274049,"user":{"displayName":"Claire Z.","userId":"15310629086553932750"},"user_tz":300},"id":"AMdAR9WZs7Fy","trusted":true},"outputs":[],"source":["NORM_corpus_test_bow = bow_vectorizer.transform(NORM_corpus_test)"]},{"cell_type":"markdown","metadata":{"id":"js87bGZhs7Fy"},"source":["We'll use the best features selected by **chi-squared statistic**. Now we need to pick up from the above Bag-of-Words matrix exactly the same best features we selected for our training dataset:"]},{"cell_type":"code","execution_count":42,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22,"status":"ok","timestamp":1649189274050,"user":{"displayName":"Claire Z.","userId":"15310629086553932750"},"user_tz":300},"id":"U_tvxXafs7Fy","outputId":"1c1c3094-3d08-46e7-c216-204cd2a61e4c","trusted":true},"outputs":[{"data":{"text/plain":["(1353, 10000)"]},"execution_count":42,"metadata":{},"output_type":"execute_result"}],"source":["NORM_corpus_test_bow_chi2_BEST = chi2_kbest.transform(NORM_corpus_test_bow)\n","NORM_corpus_test_bow_chi2_BEST.shape"]},{"cell_type":"markdown","metadata":{"id":"a1xyLtZrs7Fy"},"source":["### **Text Classification**\n","\n","We'll train a text classification model that categorizes documents into 4 classes: religion, atheism, science and computer graphics. We will use a Naive Bayes Classifier and Support Vector Machines (SVM), the last one is optional.\n","<br><br>\n","#### **Naive Bayes Classifier**"]},{"cell_type":"markdown","metadata":{"id":"K_-XX8bCs7Fz"},"source":["Make the **MultinomialNB** packages available:"]},{"cell_type":"code","execution_count":43,"metadata":{"executionInfo":{"elapsed":21,"status":"ok","timestamp":1649189274050,"user":{"displayName":"Claire Z.","userId":"15310629086553932750"},"user_tz":300},"id":"gj_0ckMUs7Fz","trusted":true},"outputs":[],"source":["from sklearn.naive_bayes import MultinomialNB"]},{"cell_type":"markdown","metadata":{"id":"DI5Oa3h4s7Fz"},"source":["Define the Naive Bayes classifier by specifiying the hyperparameter alpha and call the classifier NB_tc:<br><br>\n","*Note: you can set the hyperparameter alpha to an optimal value by trying different values > 0. With alpha = 0, you model will assign a probability of zero to a document in the test data if the document contains a feature not found in the training data.*"]},{"cell_type":"code","execution_count":44,"metadata":{"executionInfo":{"elapsed":20,"status":"ok","timestamp":1649189274050,"user":{"displayName":"Claire Z.","userId":"15310629086553932750"},"user_tz":300},"id":"Pkzcgsz8s7Fz","trusted":true},"outputs":[],"source":["NB_tc = MultinomialNB(alpha=0.1) "]},{"cell_type":"markdown","metadata":{"id":"JrsvtyIEs7Fz"},"source":["Let's train the model using the best features selected using the **chi-squared statistics**:"]},{"cell_type":"code","execution_count":45,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21,"status":"ok","timestamp":1649189274051,"user":{"displayName":"Claire Z.","userId":"15310629086553932750"},"user_tz":300},"id":"v8kP97ZLs7Fz","outputId":"ca9b231f-fb28-4581-ed87-f6ced878fb7e","trusted":true},"outputs":[{"data":{"text/html":["<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB(alpha=0.1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB(alpha=0.1)</pre></div></div></div></div></div>"],"text/plain":["MultinomialNB(alpha=0.1)"]},"execution_count":45,"metadata":{},"output_type":"execute_result"}],"source":["NB_tc.fit(NORM_corpus_train_bow_chi2_BEST, corpus_train.target)"]},{"cell_type":"code","execution_count":46,"metadata":{"executionInfo":{"elapsed":19,"status":"ok","timestamp":1649189274051,"user":{"displayName":"Claire Z.","userId":"15310629086553932750"},"user_tz":300},"id":"tgveqTQLs7Fz","trusted":true},"outputs":[],"source":["predicted_nb_chi2_best = NB_tc.predict(NORM_corpus_test_bow_chi2_BEST)"]},{"cell_type":"markdown","metadata":{"id":"6ixneC9ss7Fz"},"source":["Evaluate the predictive power for the Naive Bayes classifier using chi-squared k=10,000 best features:"]},{"cell_type":"code","execution_count":47,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19,"status":"ok","timestamp":1649189274052,"user":{"displayName":"Claire Z.","userId":"15310629086553932750"},"user_tz":300},"id":"nPdM3t45s7Fz","outputId":"4197d321-aeb9-4d68-f571-5ccd4e940367","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Confusion matrix: \n","                     alt.atheism  comp.graphics  sci.space  talk.religion.misc\n","alt.atheism                 231              7         25                  56\n","comp.graphics                15            349         23                   2\n","sci.space                    20             18        349                   7\n","talk.religion.misc           77              5         19                 150 \n","\n","Accuracy rate:  0.7974870657797487 \n","\n"]}],"source":["cm_chi2_best = metrics.confusion_matrix(corpus_test.target, predicted_nb_chi2_best)\n","print(\"Confusion matrix: \\n\", pd.DataFrame(data = cm_chi2_best , \n","                                           columns = corpus_train.target_names,\n","                                           index = corpus_train.target_names),\"\\n\")\n","print(\"Accuracy rate: \", metrics.accuracy_score(corpus_test.target, predicted_nb_chi2_best),\"\\n\") "]},{"cell_type":"markdown","metadata":{"id":"qsPTC23es7F0"},"source":["### **<font color=green>EXERCISE 4:</font>**\n","**<font color=green>4.1. Train the Naive Bayes classifier without doing feature selection, that is use all the features available in the normalized corpus. What accuracy do you get? If a classifier did a mistake and misclassified a \"Computer Graphics\" post, to which class such a post was mistakenly assigned, typically? What about a post on the \"Atheism\" topic? </font>** <br><br>\n"]},{"cell_type":"markdown","metadata":{"id":"UB2ksG6qf6eq"},"source":["I got a less accurate result. The accuracy rate is 0.79. If \"Computer Graphics\" is misclassified, typically it would be mistakenly assigned to \"science\" class. And \"Atheism\" is typically mistakenly assigned to \"religion\" class."]},{"cell_type":"code","execution_count":52,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1649191901343,"user":{"displayName":"Claire Z.","userId":"15310629086553932750"},"user_tz":300},"id":"eevqs0DteVG2","outputId":"bc7b6b60-0e9c-4c64-918a-49afceebff81"},"outputs":[{"name":"stdout","output_type":"stream","text":["Confusion matrix: \n","                     alt.atheism  comp.graphics  sci.space  talk.religion.misc\n","alt.atheism                 227              6         24                  62\n","comp.graphics                12            350         23                   4\n","sci.space                    19             18        348                   9\n","talk.religion.misc           79              8         18                 146 \n","\n","Accuracy rate:  0.7915742793791575 \n","\n"]}],"source":["NB_tc = MultinomialNB(alpha=0.1) \n","NB_tc.fit(NORM_corpus_train_bow, corpus_train.target)\n","predicted_nb = NB_tc.predict(NORM_corpus_test_bow)\n","\n","cm = metrics.confusion_matrix(corpus_test.target, predicted_nb)\n","print(\"Confusion matrix: \\n\", pd.DataFrame(data = cm , \n","                                           columns = corpus_train.target_names,\n","                                           index = corpus_train.target_names),\"\\n\")\n","print(\"Accuracy rate: \", metrics.accuracy_score(corpus_test.target, predicted_nb),\"\\n\") "]},{"cell_type":"markdown","metadata":{"id":"7Gyl_D3ndD54"},"source":["**<font color=green>4.2. (OPTIONAL) Train the Naive Bayes classifier feature selection based on mutual information (MI). What accuracy do you get?</font>** <br><br>\n","Your answer:"]},{"cell_type":"code","execution_count":51,"metadata":{"id":"JbVLAUXPs7F0"},"outputs":[{"name":"stdout","output_type":"stream","text":["Confusion matrix: \n","                     alt.atheism  comp.graphics  sci.space  talk.religion.misc\n","alt.atheism                 237              7         24                  51\n","comp.graphics                10            351         24                   4\n","sci.space                    20             18        349                   7\n","talk.religion.misc           84              7         19                 141 \n","\n","Accuracy rate:  0.7967479674796748 \n","\n"]}],"source":["NORM_corpus_test_bow_mi_BEST = MI_kbest.transform(NORM_corpus_test_bow)\n","\n","NB_tc = MultinomialNB(alpha=0.1) \n","NB_tc.fit(NORM_corpus_train_bow_mutual_BEST, corpus_train.target)\n","predicted_nb_mi_best = NB_tc.predict(NORM_corpus_test_bow_mi_BEST)\n","\n","cm_mi_best = metrics.confusion_matrix(corpus_test.target, predicted_nb_mi_best)\n","print(\"Confusion matrix: \\n\", pd.DataFrame(data = cm_mi_best , \n","                                           columns = corpus_train.target_names,\n","                                           index = corpus_train.target_names),\"\\n\")\n","print(\"Accuracy rate: \", metrics.accuracy_score(corpus_test.target, predicted_nb_mi_best),\"\\n\") "]},{"cell_type":"markdown","metadata":{"id":"lJ2auTGvs7F1"},"source":["### **<font color=green>EXERCISE 5 (OPTIONAL):</font>**\n","**<font color=green>5.1. Vectorize the data using the TF-IDF approach, with and without feature selection, and train and test the Naive Bayes classifier. What are your results? </font>** <br><br>\n","\n","Your answer:"]},{"cell_type":"markdown","metadata":{},"source":["#### without feature selection"]},{"cell_type":"code","execution_count":53,"metadata":{"id":"KMhdDGDis7F1"},"outputs":[{"name":"stdout","output_type":"stream","text":["Confusion matrix: \n","                     alt.atheism  comp.graphics  sci.space  talk.religion.misc\n","alt.atheism                 224              7         39                  49\n","comp.graphics                10            350         27                   2\n","sci.space                    19             18        354                   3\n","talk.religion.misc           84              8         20                 139 \n","\n","Accuracy rate:  0.7886178861788617 \n","\n"]}],"source":["from sklearn.feature_extraction.text import TfidfVectorizer \n","\n","vectorizer_TF_IDF = TfidfVectorizer(norm = 'l2', smooth_idf = True)\n","NORM_corpus_train_tf_idf = vectorizer_TF_IDF.fit_transform(NORM_corpus_train).todense()\n","NORM_corpus_test_tf_idf = vectorizer_TF_IDF.transform(NORM_corpus_test).todense()\n","\n","NB_tc = MultinomialNB(alpha=0.1) \n","NB_tc.fit(NORM_corpus_train_tf_idf, corpus_train.target)\n","predicted_nb = NB_tc.predict(NORM_corpus_test_tf_idf)\n","\n","cm = metrics.confusion_matrix(corpus_test.target, predicted_nb)\n","print(\"Confusion matrix: \\n\", pd.DataFrame(data = cm , \n","                                           columns = corpus_train.target_names,\n","                                           index = corpus_train.target_names),\"\\n\")\n","print(\"Accuracy rate: \", metrics.accuracy_score(corpus_test.target, predicted_nb),\"\\n\") "]},{"cell_type":"markdown","metadata":{},"source":["#### with feature selection"]},{"cell_type":"markdown","metadata":{},"source":["##### chi-square statistic"]},{"cell_type":"code","execution_count":56,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Confusion matrix: \n","                     alt.atheism  comp.graphics  sci.space  talk.religion.misc\n","alt.atheism                 221              8         38                  52\n","comp.graphics                11            351         26                   1\n","sci.space                    19             16        357                   2\n","talk.religion.misc           79              7         21                 144 \n","\n","Accuracy rate:  0.7930524759793053 \n","\n"]}],"source":["chi2_kbest = SelectKBest(score_func = chi2, k = 10000)\n","NORM_corpus_train_tf_idf_chi2_BEST = chi2_kbest.fit_transform(NORM_corpus_train_tf_idf, corpus_train.target)\n","NORM_corpus_test_tf_idf_chi2_BEST = chi2_kbest.transform(NORM_corpus_test_tf_idf)\n","\n","NB_tc = MultinomialNB(alpha=0.1) \n","NB_tc.fit(NORM_corpus_train_tf_idf_chi2_BEST, corpus_train.target)\n","predicted_nb_chi2_best = NB_tc.predict(NORM_corpus_test_tf_idf_chi2_BEST)\n","\n","cm_chi2_best = metrics.confusion_matrix(corpus_test.target, predicted_nb_chi2_best)\n","print(\"Confusion matrix: \\n\", pd.DataFrame(data = cm_chi2_best, \n","                                           columns = corpus_train.target_names,\n","                                           index = corpus_train.target_names),\"\\n\")\n","print(\"Accuracy rate: \", metrics.accuracy_score(corpus_test.target, predicted_nb_chi2_best),\"\\n\") "]},{"cell_type":"markdown","metadata":{},"source":[]}],"metadata":{"colab":{"collapsed_sections":[],"name":"Lab_3_SP2022_Text_Classification.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3.9.9 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.9"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"}}},"nbformat":4,"nbformat_minor":0}
