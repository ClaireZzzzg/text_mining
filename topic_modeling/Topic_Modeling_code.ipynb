{"cells":[{"cell_type":"markdown","metadata":{"id":"DTXuzy_vFWWH"},"source":["# TOPIC MODELING (via Latent Dirichlet Allocation)"]},{"cell_type":"markdown","metadata":{},"source":["## Import packages"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"ByGh528eFWWK"},"outputs":[],"source":["#the module 'sys' allows istalling module from inside Jupyter\n","import sys\n","\n","!{sys.executable} -m pip install numpy\n","import numpy as np\n","\n","!{sys.executable} -m pip install pandas\n","import pandas as pd\n","\n","#Natrual Language ToolKit (NLTK)\n","!{sys.executable} -m pip install nltk\n","import nltk\n","\n","!{sys.executable} -m pip install sklearn\n","from sklearn import metrics\n","#from sklearn.model_selection import GridSearchCV\n","from sklearn.feature_extraction.text import  CountVectorizer #bag-of-words vectorizer \n","from sklearn.decomposition import LatentDirichletAllocation #package for LDA\n","\n","# Plotting tools\n","\n","from pprint import pprint\n","!{sys.executable} -m pip install pyLDAvis #visualizing LDA\n","import pyLDAvis\n","import pyLDAvis.sklearn\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","#define text normalization function\n","%run ./Text_Normalization_Function.ipynb #defining text normalization function\n","\n","#ignore warnings about future changes in functions as they take too much space\n","import warnings\n","warnings.simplefilter(action='ignore', category=FutureWarning)\n","warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n","\n","#calcualte coherence score for evaluating LDA model\n","#The sklearn module does not have the functionality to compute the coherence score. Let's install the gensim package and the functions needed\n","!{sys.executable} -m pip install gensim\n","import gensim\n","from gensim.models.coherencemodel import CoherenceModel\n","from gensim.corpora.dictionary import Dictionary\n","     "]},{"cell_type":"markdown","metadata":{"id":"pbNFzhvFFWWQ"},"source":["## Code \n"]},{"cell_type":"markdown","metadata":{},"source":["### Calculate coherence score"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def coherence_score(normalized_corpus,lda_model,metric = 'u_mass'):\n","     \n","     #tokenizing the corpus\n","     news_corpus_tokenized = [tokenize_text(normalized_corpus[doc_id]) for doc_id in range(len(normalized_corpus))]\n","\n","     #Dictionary of the corpus:\n","     news_dictionary = Dictionary(news_corpus_tokenized)\n","\n","     #Bag-of-words representation for each document of the corpus:\n","     news_corpus_bow = [news_dictionary.doc2bow(doc) for doc in news_corpus_tokenized]\n","\n","     def get_topic_words(vectorizer, lda_model, n_words):\n","         keywords = np.array(vectorizer.get_feature_names())\n","         topic_words = []\n","         for topic_weights in lda_model.components_:\n","             top_word_locs = (-topic_weights).argsort()[:n_words]\n","             topic_words.append(keywords.take(top_word_locs).tolist())\n","         return topic_words\n","\n","     #top 20 words for each topic (using the function defined in session prep)\n","     topic_topwords = get_topic_words(vectorizer = bow_vectorizer_news, lda_model = lda_model, n_words=20)\n","     \n","     cm = CoherenceModel(topics=topic_topwords, #An array of top words for each topic\n","                         corpus = news_corpus_bow , #Corpus with each document represented as Bag-of-Words\n","                         dictionary = news_dictionary, #Dictionary of the corpus\n","                         coherence = metric)\n","     #We use one of the coherence metrics \"u-mass\" which measures semantic similarity of words in a topic, but there are other metrics as well.\n","     #*Note: You can check out different coherence metrics here if you are interested: https://dl.acm.org/doi/abs/10.1145/2684822.2685324*\n","\n","     #print(\"Coherence score for the model: \", np.round(cm.get_coherence(), 4))  # get coherence value\n","     #print(\"Coherence score by topic (higher values are better): \", np.round(cm.get_coherence_per_topic(),4))\n","     \n","     return np.round(cm.get_coherence(), 4)"]},{"cell_type":"markdown","metadata":{},"source":["### Select topic number, build & evaluate the LDA model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def lda_score(normalized_corpus,vectorized_corpus,topic_num,max_iter=100,doc_topic_prior = 0.25,topic_word_prior = 0.25,metric = 'u_mass'):\n","     \n","     #build lda model\n","     lda_model = LatentDirichletAllocation(n_components=topic_num, \n","                                          max_iter=max_iter,\n","                                          doc_topic_prior = doc_topic_prior,\n","                                          topic_word_prior = topic_word_prior).fit(vectorized_corpus)\n","     \n","     log_likelihood = lda_model.score(vectorized_corpus)\n","     perplexity = lda_model.perplexity(vectorized_corpus)\n","     coherence = coherence_score(normalized_corpus,lda_model,metric)\n","     \n","     return log_likelihood,perplexity,coherence\n","     "]},{"cell_type":"markdown","metadata":{},"source":["### Use forloop to build several LDA models and compare performances on three metrics"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def lda_models(lower_value,upper_value,normalized_corpus,vectorized_corpus):\n","     \n","     chart = pd.DataFrame()\n","          \n","     for i in range(lower_value,upper_value):\n","          chart[\"model{0}\".format(i)] = [round(i,4) for i in list(lda_score(normalized_corpus_news,bow_news_corpus,i))]\n","     \n","     ind = pd.Series(['log_likelihood_score','perplexity_score','coherence_score'])\n","     chart = chart.set_index(ind)\n","\n","     ind_lst = []\n","     for index in chart.index:\n","          if index in ('log_likelihood_score','coherence_score'):\n","               best_model_ind = np.argmax(chart.loc[index,:].values)\n","          else:\n","               best_model_ind = np.argmin(chart.loc[index,:].values)\n","               \n","          ind_lst.append(best_model_ind)\n","     \n","     chart['best_model'] = [chart.columns[ind] for ind in ind_lst]\n","     \n","     return chart"]},{"cell_type":"markdown","metadata":{},"source":["### Check the words distribution in each cluster"]},{"cell_type":"markdown","metadata":{},"source":["If you gonna select a model you'd like to dive deeper, please use the code below."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def display_topics(model, feature_names, no_top_words):\n","          for topic_idx, topic in enumerate(model.components_):\n","               print(\"Topic %d:\" % (topic_idx))\n","               print(\" \".join([feature_names[i]\n","                        for i in topic.argsort()[:-no_top_words - 1:-1]]))"]},{"cell_type":"markdown","metadata":{},"source":["Or if you trust the metric's measurement, just use the code below to look into the best LDA model chosed by 3 metrics."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def display_best_model_topic(chart,vectorized_corpus,vectorizer,num_top_words):\n","     best_lst = list(chart.best_model)\n","     best_model = max(best_lst,key=best_lst.count) \n","     optimal_topic_num = int(best_model[-1])\n","     \n","     lda_model = LatentDirichletAllocation(n_components=optimal_topic_num, \n","                                     max_iter=100,\n","                                     doc_topic_prior = 0.25,\n","                                     topic_word_prior = 0.25).fit(vectorized_corpus)\n","     \n","     return display_topics(lda_model,vectorizer.get_feature_names(),num_top_words)"]},{"cell_type":"markdown","metadata":{},"source":["# Data Example"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# from sklearn.datasets import fetch_20newsgroups\n","# categories = ['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']\n","# dataset = fetch_20newsgroups(shuffle=True, \n","#                              random_state=1, \n","#                              categories = categories, \n","#                              remove=('headers', 'footers', 'quotes'))\n","\n","# news_corpus = dataset.data\n","\n","# #normalize data\n","# normalized_corpus_news = normalize_corpus(news_corpus)\n","\n","# #define a Bag-of-Words vecgtorizer\n","# bow_vectorizer_news = CountVectorizer(max_features=1000)\n","\n","# #vectorize data\n","# bow_news_corpus = bow_vectorizer_news.fit_transform(normalized_corpus_news)"]},{"cell_type":"code","execution_count":117,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>model2</th>\n","      <th>model3</th>\n","      <th>model4</th>\n","      <th>best_model</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>log_likelihood_score</th>\n","      <td>-751912.6647</td>\n","      <td>-743382.2788</td>\n","      <td>-741007.5149</td>\n","      <td>model4</td>\n","    </tr>\n","    <tr>\n","      <th>perplexity_score</th>\n","      <td>629.2006</td>\n","      <td>584.8401</td>\n","      <td>573.0568</td>\n","      <td>model4</td>\n","    </tr>\n","    <tr>\n","      <th>coherence_score</th>\n","      <td>-1.5082</td>\n","      <td>-1.4471</td>\n","      <td>-1.4562</td>\n","      <td>model3</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                           model2       model3       model4 best_model\n","log_likelihood_score -751912.6647 -743382.2788 -741007.5149     model4\n","perplexity_score         629.2006     584.8401     573.0568     model4\n","coherence_score           -1.5082      -1.4471      -1.4562     model3"]},"execution_count":117,"metadata":{},"output_type":"execute_result"}],"source":["# test\n","# lda_models(2,5,normalized_corpus_news,bow_news_corpus)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Topic 0:\n","think know people like jesus good could time thing god even take well give way\n","Topic 1:\n","space nasa launch satellite system orbit year use mission earth shuttle data lunar program moon\n","Topic 2:\n","image file use edu program software graphic format jpeg ftp data available color mail system\n","Topic 3:\n","god people believe atheist religion think argument atheism exist many christian use point belief must\n"]}],"source":["# display_best_model_topic(chart,bow_news_corpus,bow_vectorizer_news,15)"]},{"cell_type":"markdown","metadata":{"id":"15az3h_rFWWe"},"source":["<br>**NOTE:** The script can vary both parameters of the Dirichlet distributions and the number of topics, or just the number of topics. In this script, I just use number of topics as a parameter, so later you could add Dirichlet parameters according to your perference."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# #run this visualization on Colab notebook\n","# #prepare to display result in the Jupyter notebook\n","# pyLDAvis.enable_notebook()\n","\n","# #run the visualization [mds is a function to use for visualizing the \"distance\" between topics]\n","# pyLDAvis.sklearn.prepare(lda_news, bow_news_corpus, bow_vectorizer_news, mds='tsne')"]}],"metadata":{"colab":{"collapsed_sections":["zAoBYcHMFWWN","ajS_YRPuFWWT"],"name":"Lab_4_Topic_Modeling_SP2022.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3.9.9 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.9"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"}}},"nbformat":4,"nbformat_minor":0}
